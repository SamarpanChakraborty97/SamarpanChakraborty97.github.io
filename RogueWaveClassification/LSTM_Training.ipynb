{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d133c4-59b5-4ec2-bf99-d4c168b27685",
   "metadata": {},
   "source": [
    "## This notebook is intended to train a LSTM model using the different data generated for different classification scenarios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd0985-cbd2-4b50-bf61-c2c913c9bd04",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6471af5-9b3a-4415-a396-dd4e2b3f630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703f914-5986-40ce-b96f-e19c0f51e2c5",
   "metadata": {},
   "source": [
    "#### Timing the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2f9fd-be37-47de-a93d-7ad5665904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78820b0d-4a6a-4616-9496-c4f3d7da061d",
   "metadata": {},
   "source": [
    "#### Filenames to be used for storing models, training process and the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fef2d5-e529-4853-bc3b-b82571cf90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_str=\"RWs_H_g_2_tadv_10min_deep_buoys_rw_0.5\"\n",
    "file_str_test = \"RWs_H_g_2_tadv_10min_deep_buoys_test_rw_0.5\"\n",
    "\n",
    "LSTM_save_name= os.getcwd()+ \"/model_saves\" + \"/\" + \"best_LSTM_\"+file_str +\"checkpoint.model.keras\"\n",
    "metrics_save_name = os.getcwd() + \"/metric_saves_lstm\" + \"/\" + file_str + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f902808-6f49-4664-96ff-92bdbca6600d",
   "metadata": {},
   "source": [
    "#### Creating the train and test data for training the LSTM model.\n",
    "**The training data for different scenarios can be accessed here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc78f4-1fd7-4b4a-b625-dc7b02e1b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(file_str+\".npz\")\n",
    "data_test = np.load(file_str_test+\".npz\")\n",
    "\n",
    "for vars in data:\n",
    "    print(vars)\n",
    "\n",
    "wave_data_train=data[\"wave_data_train\"]\n",
    "wave_data_test=data_test[\"wave_data_test\"]\n",
    "label_train=data[\"label_train\"]\n",
    "label_test=data_test[\"label_test\"]\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "print(wave_data_train.shape)\n",
    "print(wave_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d20ef7-ace0-4a18-b92c-76e504801f98",
   "metadata": {},
   "source": [
    "**Creating the LSTM model using Tensorflow. Callbacks were created to enable early stopping and learning rate schedulers were put in place to schedule the learning rate with epoch progression.**\n",
    "**The different hyperparameters used here were chosen after hyperparameter tuning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d309e5-edf0-44ba-9d65-3edfa4c100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "model_LSTM = keras.Sequential()\n",
    "model_LSTM.add(keras.layers.LSTM(10, input_shape = wave_data_train.shape[1:], return_sequences=True))\n",
    "model_LSTM.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model_LSTM.add(keras.layers.LSTM(10, return_sequences=True)) #, return_sequences=True\n",
    "model_LSTM.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model_LSTM.add(keras.layers.LSTM(10, return_sequences=True))#, return_sequences=True\n",
    "model_LSTM.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model_LSTM.add(keras.layers.LSTM(10))\n",
    "model_LSTM.add(keras.layers.BatchNormalization())\n",
    "model_LSTM.add(keras.layers.Dropout(0.05))\n",
    "model_LSTM.add(keras.layers.Dense(num_classes, activation=\"sigmoid\"))#\n",
    "\n",
    "\n",
    "model_LSTM.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"sparse_categorical_accuracy\"],\n",
    "            optimizer=\"adam\")\n",
    "\n",
    "model_LSTM.summary()\n",
    "\n",
    "import math\n",
    "def scheduler(epochs, lr):\n",
    "    if epochs < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        LSTM_save_name, save_best_only=True, monitor=\"val_loss\"#\"val_sparse_categorical_accuracy\"#\"val_loss\"\n",
    "    ),\n",
    "   \n",
    "    keras.callbacks.LearningRateScheduler(scheduler),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, verbose=0), #\"val_loss\"\n",
    "]\n",
    "\n",
    "history_LSTM=model_LSTM.fit(wave_data_train, label_train,batch_size=batch_size,   epochs=500, validation_split=0.2, callbacks=callbacks, \n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bc8aa-3c47-4e9a-96d5-fd62879073e7",
   "metadata": {},
   "source": [
    "**Function to generate the training curves and time the training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff497303-0a7c-4759-94cc-5a1074fdfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"sparse_categorical_accuracy\"\n",
    "save_history_plot=True\n",
    "if save_history_plot ==True:\n",
    "    plt.figure()\n",
    "    plt.plot(history_LSTM.history[metric])\n",
    "    plt.plot(history_LSTM.history[\"val_\" + metric])\n",
    "    plt.title(\"model \" + metric)\n",
    "    plt.ylabel(metric, fontsize=\"large\")\n",
    "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "    plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "    filename=os.getcwd()+'/training_history_'+'/'+file_str+'.jpg'\n",
    "    print(filename)\n",
    "    plt.savefig(filename,dpi=199)\n",
    "    plt.close()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time is {elapsed_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03fdf5-4961-4af2-9ce6-2327e4c8a09e",
   "metadata": {},
   "source": [
    "**A sample plot depicting the training and validation accuracies are plotted here.**\n",
    "<img src=\"training_RWs_H_g_2_tadv_5min_rw_smallWindow_0.5.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9679798-3cd0-4369-bdd1-bd80944dea22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
