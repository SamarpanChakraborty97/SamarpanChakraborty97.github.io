{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook intends to create a graph convolution network to be used for graph-level classification of rogue waves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This rogue wave classification task is based upon our previously segregated wave groups based on wave magnitudes. For this graph-level classification task, we intend to build a graph structure where each graph is a separate data sample and each node of this graph is time entry of this sample.**\n",
    "- **The edges of this graph structure can be built by leveraging the proximity between the different time series entries for each sample. This requires the number of neighbours (neighboring time entries in this case) which can be treated as a hyperparameter for our training process.**\n",
    "- **Each node has a single feature (the value of the time series entry.)**\n",
    "- **This node information and edge information for each graph is utilized for the learning of the graph structure whose task is to classify whether the graph structure is pointing to the presence or absence of an extreme wave at the end. The different data samples are individual graphs, and similar to training other neural networks, learning the graph structure involves learning across these graphs in mini-batches and trying to reduce the loss according to the chosen loss function.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"GNN_overview.jpg\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "**A sample graph structure showing node information, edge embeddings and global embeddings is shown here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, Dropout, ReLU, Sigmoid\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed for reproduciiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Seed for the random module\n",
    "    np.random.seed(seed)  # Seed for NumPy\n",
    "    torch.manual_seed(seed)  # Seed for PyTorch\n",
    "    torch.cuda.manual_seed(seed)  # Seed for current GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # Seed for all GPUs (if you have more than one)\n",
    "\n",
    "# Set the desired seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The forecasting of rogue waves is undertaken by designing the task as follows.**\n",
    "- **Given a window of time series data extracted from a buoy, the purpose of the task is to predict whether there will be a rogue wave within some fixed time horizon. The training data is prepared such that there are equal proportions of wave data windows leading to a rogue wave in the horizon and those that do not lead up to a rogue wave in the horizon.**,\n",
    "- **The training input is thus each such data window, while the output is determined by the presence or absence of a rogue wave at the end of the fixed forecasting horizon.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"Slide3.jpg\" width=\"600\"; height=\"300\">\n",
    "</div>\n",
    "\n",
    "**An overview of the data window used and the subsequent rogue wave to be forecast is displayed through the illustration here. The forecast horizon $t_{horizon}$ is 5 minutes and the length of the training window $t_{window}$ is fixed at 15 minutes in our set of classification efforts for this case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following this, the training window data is converted into a graph structure and stored for using as input to the graph neural networks being trained here. This is done [here](./Data_Preparation_different_wave_groups.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the datafile and prepare the data in the required graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------THE GRAPH STRUCTURE WILL BE IN THE FORM OF MULTIPLE GRAPHS------------------\n",
      "--EACH GRAPH WILL HAVE MULTIPLE NODES WHERE EACH NODE IS AN OBSERVATION OF THE TIME SERIES--\n",
      "----------------------------THE FEATURE LENGTH OF EACH NODE IS 1----------------------------\n",
      "\n",
      "\n",
      "-----------------------THE GRAPH PROPERTIES ARE GIVEN BELOW---------------------\n",
      "\n",
      "The number of nodes in each graph is 1152\n",
      "The number of features for each node is 1\n",
      "The number of training set examples is 6312\n",
      "The number of test set examples is 2706\n",
      "The number of neighbours chosen for each node is 70\n"
     ]
    }
   ],
   "source": [
    "file_str=\"tadv_5min_wave_group_window_15mins_4\"\n",
    "\n",
    "data=np.load('wave_groups/'+file_str+\".npz\")\n",
    "wave_data_train=data[\"wave_data_train\"]\n",
    "wave_data_test=data[\"wave_data_test\"]\n",
    "label_train=data[\"label_train\"]\n",
    "label_test=data[\"label_test\"]\n",
    "num_classes=2\n",
    "\n",
    "\n",
    "num_nodes = wave_data_train.shape[1]\n",
    "num_node_features = wave_data_train.shape[2]\n",
    "num_neighbours = 70  ### Choice for the number of neighbours in the graph structure\n",
    "batch_size = 32  ### Batch size to be used for creating dataloaders\n",
    "\n",
    "print(\"----------------The graph structure will be in the form of multiple graphs------------------\".upper())\n",
    "print(\"--Each graph will have multiple nodes where each node is an observation of the time series--\".upper())\n",
    "print(\"----------------------------The feature length of each node is 1----------------------------\".upper())\n",
    "print('\\n')\n",
    "print('-----------------------The graph properties are given below---------------------\\n'.upper())\n",
    "print(f\"The number of nodes in each graph is {num_nodes}\")\n",
    "print(f\"The number of features for each node is {num_node_features}\")\n",
    "print(f\"The number of training set examples is {wave_data_train.shape[0]}\")\n",
    "print(f\"The number of test set examples is {wave_data_test.shape[0]}\")\n",
    "print(f\"The number of neighbours chosen for each node is {num_neighbours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the sparse adjacency matrix and print out the details required for constructing the edge matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The matrix is computed based on the number of neighbors chosen.**\n",
    "- **Options for directionality is provided. Thus, it can be decided if previous time series entries can affect the matrix formation or not.**\n",
    "- **Based on a chosen number of neighbors, an edge is created between nodes if it is close enough to the node in question. That is, if the two time series entries are within the chosen number of neighbors, an edge is created between them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------DETAILS REQUIRED FOR CONSTRUCTING THE EDGE MATRICES-----------\n",
      "\n",
      "Are the edges directed? False\n",
      "The number of edges in each graph is 80640\n"
     ]
    }
   ],
   "source": [
    "def create_adjacency_matrix(num_nodes, directionality=True):\n",
    "    D_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            D_matrix[i,j] = abs(i-j)\n",
    "\n",
    "    k = num_neighbours\n",
    "    A = np.zeros_like(D_matrix)\n",
    "    for i in range(len(A)):\n",
    "        neighbours = np.argsort(D_matrix[i])[:k]\n",
    "        if directionality:\n",
    "            for n in neighbours:\n",
    "                if n>i:\n",
    "                    A[i, n] = 0\n",
    "                else:\n",
    "                    A[i, n] = 1 \n",
    "        else:\n",
    "            A[i, neighbours] = 1 \n",
    "\n",
    "    return A\n",
    "\n",
    "directionality = False\n",
    "A = create_adjacency_matrix(num_nodes, directionality)\n",
    "sparse_matrix = sp.coo_matrix(A)\n",
    "indices = np.column_stack((sparse_matrix.nonzero()))\n",
    "values = sparse_matrix.data\n",
    "dense_shape = sparse_matrix.shape\n",
    "sparse_A = tf.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
    "\n",
    "print(\"---------Details required for constructing the edge matrices-----------\\n\".upper())\n",
    "print(f\"Are the edges directed? {directionality}\")\n",
    "print(f\"The number of edges in each graph is {len(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For the weights of the edges created, they are determined based on the proximity of the neighbors.**\n",
    "- **Thus, for the same node, the edge weight is 1, and as the nodes become distant from each other, the edge weight decreases proportionately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the edge weights in a file for further use and use if already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of the edge weights for training is: (6312, 80640)\n",
      "The shape of the edge weights for testing is: (2706, 80640)\n"
     ]
    }
   ],
   "source": [
    "edge_file_path = os.getcwd() + f\"\\edge_data_15_4_original_num_neighbors_{num_neighbours}.npz\"\n",
    "\n",
    "if os.path.exists(edge_file_path):\n",
    "    edge_data=np.load(edge_file_path)\n",
    "    edge_data_train=edge_data[\"edge_data_train\"]\n",
    "    edge_data_test=edge_data[\"edge_data_test\"]\n",
    "\n",
    "else:\n",
    "    edge_features_train = np.zeros((wave_data_train.shape[0], len(indices)))\n",
    "    edge_features_test = np.zeros((wave_data_test.shape[0], len(indices)))\n",
    "\n",
    "    for i in range(edge_features_train.shape[0]):\n",
    "        for j in range(len(indices)):\n",
    "            index = indices[j]\n",
    "            edge_features_train[i,j] = abs(index[1]-index[0])\n",
    "\n",
    "    for i in range(edge_features_test.shape[0]):\n",
    "        for j in range(len(indices)):\n",
    "            index = indices[j]\n",
    "            edge_features_test[i,j] = abs(index[1]-index[0])\n",
    "\n",
    "    train_min = np.min(edge_features_train)\n",
    "    train_max = np.max(edge_features_train)\n",
    "\n",
    "    test_min = np.min(edge_features_test)\n",
    "    test_max = np.max(edge_features_test)\n",
    "\n",
    "    edge_features_train = (edge_features_train - train_min) / (train_max - train_min)\n",
    "    edge_features_test = (edge_features_test - test_min) / (test_max - test_min)\n",
    "\n",
    "    np.savez(edge_file_path,edge_data_train=edge_features_train, edge_data_test=edge_features_test)\n",
    "\n",
    "    edge_data=np.load(edge_file_path)\n",
    "    edge_data_train=edge_data[\"edge_data_train\"]\n",
    "    edge_data_test=edge_data[\"edge_data_test\"]\n",
    "\n",
    "print(f\"\\nThe shape of the edge weights for training is: {edge_data_train.shape}\")\n",
    "print(f\"The shape of the edge weights for testing is: {edge_data_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the graphs using the data above, creating the graph dataset and saving it for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_graphs(edge_tensor, node_tensor, labels, indices, path):\n",
    "\n",
    "    graphs = []  \n",
    "    edge_data = torch.tensor([indices[:,0], indices[:,1]])\n",
    "        \n",
    "    num_graphs = len(node_tensor)\n",
    "    for k in range(num_graphs):\n",
    "        x = node_tensor[k]\n",
    "        edge_weights = edge_tensor[k]\n",
    "        y = labels[k]\n",
    "\n",
    "        graph = Data(x=x, edge_index=edge_data, edge_weight=edge_weights, y=y)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    torch.save(graphs, path)\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating or loading the training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samar\\AppData\\Local\\Temp\\ipykernel_5556\\2776663255.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load(train_data_path)\n",
      "C:\\Users\\samar\\AppData\\Local\\Temp\\ipykernel_5556\\2776663255.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load(val_data_path)\n",
      "C:\\Users\\samar\\AppData\\Local\\Temp\\ipykernel_5556\\2776663255.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load(test_data_path)\n"
     ]
    }
   ],
   "source": [
    "### Creating the training, validation and the test datasets\n",
    "train_val_split = 0.7\n",
    "\n",
    "### The paths to the saved datasets\n",
    "train_data_path = f'train_set_original_length_num_neighbours_{num_neighbours}_graph_classification.pth'\n",
    "val_data_path = f'val_set_original_length_graph_num_neighbours_{num_neighbours}_classification.pth'\n",
    "test_data_path = f'test_set_original_length_graph_num_neighbours_{num_neighbours}_classification.pth'\n",
    "\n",
    "if os.path.exists(train_data_path) and os.path.exists(val_data_path) and os.path.exists(test_data_path):\n",
    "    train_dataset = torch.load(train_data_path)\n",
    "    val_dataset = torch.load(val_data_path)\n",
    "    test_dataset = torch.load(test_data_path)\n",
    "\n",
    "else:\n",
    "    train_dataset = create_and_save_graphs(edge_data_train[:int(train_val_split * len(edge_data_train)),:], wave_data_train[:int(train_val_split * len(wave_data_train)),:], label_train[:int(train_val_split * len(label_train))], indices, train_data_path)\n",
    "    val_dataset = create_and_save_graphs(edge_data_train[int(train_val_split * len(edge_data_train)):,:], wave_data_train[int(train_val_split * len(wave_data_train)):,:], label_train[int(train_val_split * len(label_train)):], indices, val_data_path)\n",
    "    test_dataset = create_and_save_graphs(edge_data_test, wave_data_test, label_test, indices, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------------CONTAINS THE DATASET INFORMATION FOR THE TRAINING, VALIDATION AND TEST DATA--------------------\n",
      "\n",
      "THE TRAINING DATASET HAS 4418 GRAPHS\n",
      "THE VALIDATION DATASET HAS 1894 GRAPHS\n",
      "THE TESTING DATASET HAS 2706 GRAPHS\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n ----------------contains the dataset information for the training, validation and test data--------------------\\n\".upper())\n",
    "print(f\"The training dataset has {len(train_dataset)} graphs\".upper())\n",
    "print(f\"The validation dataset has {len(val_dataset)} graphs\".upper())\n",
    "print(f\"The testing dataset has {len(test_dataset)} graphs\".upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataloaders for faster data retrieval during model training and adding batch info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Node Features (x) shape:\n",
      "(1152, 1)\n",
      "Batch Labels (y) shape:\n",
      "torch.Size([32])\n",
      "\n",
      "Batch Edge Indices (edge_index) shape:\n",
      "torch.int64\n",
      "\n",
      "Batch Edge Weights (edge_weight) shape:\n",
      "(80640,)\n",
      "\n",
      "Batch Information (batch) shape:\n",
      "torch.Size([36864])\n",
      "\n",
      "Batch Size: 1152 nodes\n",
      "Number of Graphs in this Batch: 32\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Printing the info of a batch \n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch Node Features (x) shape:\")\n",
    "    print(batch.x[0].shape)  # Batched node features\n",
    "    print(\"Batch Labels (y) shape:\")\n",
    "    print(batch.y.shape)  # Batched node features\n",
    "    print(\"\\nBatch Edge Indices (edge_index) shape:\")\n",
    "    print(batch.edge_index.to(torch.int64).dtype)  # Batched edge indices\n",
    "    print(\"\\nBatch Edge Weights (edge_weight) shape:\")\n",
    "    print(batch.edge_weight[0].shape)  # Batched edge weights\n",
    "    print(\"\\nBatch Information (batch) shape:\")\n",
    "    print(batch.batch.shape)  # Batch info: indicates graph membership of each node\n",
    "    print(\"\\nBatch Size:\", len(batch.x[0]), \"nodes\")\n",
    "    print(\"Number of Graphs in this Batch:\", batch.batch.max().item() + 1)\n",
    "    # print((torch.tensor(batch.x)).reshape(len(batch.x) * len(batch.x[0]),1).shape)\n",
    "    # print((torch.tensor(batch.edge_weight)).reshape(len(batch.x) * len(batch.edge_weight[0])).shape)\n",
    "    break  # Exit after the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Graph Convolution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The graph convolution model is composed of three modules.**\n",
    "- **Initially, the number of features for each node is increased by passing through MLP layers.**\n",
    "- **The output from the MLP layers is then passed through layers of graph convolutional layers.**\n",
    "- **The outputs from the graph layers are pooled globally before feeding to a linear layer with sigmoid activation for the final classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionalNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a gnn model based on global pooling of embeddings - for graph level classifications\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_pre_MLP, dim_post_MLP, dim_graphLin, num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training=True):\n",
    "        \n",
    "        self.dim_pre_MLP = dim_pre_MLP\n",
    "        self.dim_post_MLP = dim_post_MLP\n",
    "        self.dim_graphLin = dim_graphLin\n",
    "        self.num_pre_layers = num_pre_layers\n",
    "        self.num_post_layers = num_post_layers\n",
    "        self.num_graph_layers = num_graph_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.training = training\n",
    "\n",
    "        super(GraphConvolutionalNetwork, self).__init__()\n",
    "        self.relu = ReLU()\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "        ### Each MLP module has two linear layers, each followed by activation functions of ReLU\n",
    "        self.MLP = nn.ModuleList()\n",
    "        for i in range(self.num_pre_layers):\n",
    "            if i==0:\n",
    "                mlp_layer = Sequential(Linear(num_node_features, self.dim_pre_MLP), ReLU()) \n",
    "                self.MLP.append(mlp_layer)\n",
    "            else:\n",
    "                mlp_layer = Sequential(Linear(self.dim_pre_MLP , self.dim_pre_MLP), ReLU()) \n",
    "                self.MLP.append(mlp_layer)\n",
    "                    \n",
    "        ### The MLP layers is followed by graph convolutional layers\n",
    "        self.graphLayers = nn.ModuleList()\n",
    "        for i in range(self.num_graph_layers):\n",
    "            if i==0:\n",
    "                gconv_layer = GCNConv(self.dim_pre_MLP, self.dim_graphLin)\n",
    "                self.graphLayers.append(gconv_layer)\n",
    "            else:\n",
    "                gconv_layer = GCNConv(self.dim_graphLin, self.dim_graphLin)\n",
    "                self.graphLayers.append(gconv_layer)\n",
    "        \n",
    "        ### The graph convolutional layers are followed by post processing layers\n",
    "        ### Each MLP module has two linear layers, each followed by activation functions of ReLU\n",
    "        self.postGCLayers = nn.ModuleList()\n",
    "        for i in range(self.num_post_layers):\n",
    "            if i==0 and i!=self.num_post_layers-1:\n",
    "                if self.training:\n",
    "                    mlp_layer = Sequential(Linear(self.dim_graphLin, self.dim_post_MLP), Dropout(self.dropout_prob), ReLU())\n",
    "                else:\n",
    "                    mlp_layer = Sequential(Linear(self.dim_graphLin, self.dim_post_MLP), ReLU())\n",
    "                self.postGCLayers.append(mlp_layer)\n",
    "\n",
    "            elif i==0 and i==self.num_post_layers-1:\n",
    "                mlp_layer = Linear(self.dim_graphLin, 1)\n",
    "                self.postGCLayers.append(mlp_layer)\n",
    "\n",
    "            elif i>0 and i<self.num_post_layers-1:\n",
    "                if self.training:\n",
    "                    mlp_layer = Sequential(Linear(self.dim_post_MLP, self.dim_post_MLP), Dropout(self.dropout_prob), ReLU())\n",
    "                else:\n",
    "                    mlp_layer = Sequential(Linear(self.dim_post_MLP, self.dim_post_MLP), ReLU())\n",
    "                self.postGCLayers.append(mlp_layer)\n",
    "                \n",
    "            else:\n",
    "                mlp_layer = Linear(self.dim_post_MLP, 1)\n",
    "                self.postGCLayers.append(mlp_layer)\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_weights, batch):\n",
    "        ## Pre-processing layers (MLP modules)\n",
    "        # print(batch)\n",
    "        # print(f\"The number of unique batches in the batch tensor is {len(torch.unique(batch))}\")\n",
    "        for i in range(self.num_pre_layers):\n",
    "            x = self.MLP[i](x)\n",
    "\n",
    "        # print(f\"The shape of the input at the end of the pre-processing MLP layers is {x.shape}\")\n",
    "        # print(f\"The shape of the the weights fed to the model is {edge_weights.shape}\")\n",
    "        ## Node embeddings\n",
    "        for k in range(self.num_graph_layers):\n",
    "            x = self.graphLayers[k](x, edge_indices, edge_weights)\n",
    "            x = self.relu(x)\n",
    "            # print(f\"The shape of the embedding at the end of the graph layer {k} is {x.shape}\")\n",
    "\n",
    "        ## Graph-level pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(f\"The shape of the embedding after graph level global pooling is {h[0].shape}\")\n",
    "\n",
    "        ### Post-processing and classification\n",
    "        if self.num_post_layers == 1:\n",
    "            x = self.sigmoid(self.postGCLayers[i](x))\n",
    "        else:\n",
    "            for i in range(self.num_post_layers):\n",
    "                if i < self.num_post_layers-1:\n",
    "                    x = self.postGCLayers[i](x)\n",
    "                    # print(f\"The shape of the input at the end of the post-processing MLP layer {i} is {x.shape}\")\n",
    "                else:\n",
    "                    x = self.sigmoid(self.postGCLayers[i](x))\n",
    "                    # print(f\"The shape of the input at the end of the post-processing MLP layer {i} is {x.shape}\")\n",
    "        # print(f\"The shape of the input at the end of the post-processing MLP layer {i} is {h_concat.shape}\")\n",
    "\n",
    "        # print(f\"The shape of the input at the end of the post-processing MLP layers is {h_concat.shape}\")\n",
    "        # print(f\"The output is:{h_concat}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since introducing many layers in the module might result in overfitting, options for skipping connections are also introduced in this module to reduce overfitting .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionNetworkWithSkipConnections(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a gnn model based on global pooling of embeddings - for graph level classifications\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_pre_MLP, dim_post_MLP, dim_graphLin, num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training=True):\n",
    "        \n",
    "        self.dim_pre_MLP = dim_pre_MLP\n",
    "        self.dim_post_MLP = dim_post_MLP\n",
    "        self.dim_graphLin = dim_graphLin \n",
    "        self.num_pre_layers = num_pre_layers  ## Now, it gives the number of dense layers\n",
    "        self.num_post_layers = num_post_layers  ## Now, it gives the number of dense layers\n",
    "        self.num_graph_layers = num_graph_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.training = training\n",
    "\n",
    "        super(GraphConvolutionNetworkWithSkipConnections, self).__init__()\n",
    "        self.MLP = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for i in range(self.num_pre_layers):\n",
    "            if i==0:\n",
    "                linear_layer = Linear(num_node_features, self.dim_pre_MLP)\n",
    "                self.MLP.append(linear_layer)\n",
    "            else:\n",
    "                linear_layer = Linear(self.dim_pre_MLP + num_node_features, self.dim_pre_MLP)\n",
    "                self.MLP.append(linear_layer)\n",
    "                    \n",
    "        ### The MLP layers is followed by graph convolutional layers\n",
    "        self.graphLayers = nn.ModuleList()\n",
    "        for i in range(self.num_graph_layers):\n",
    "            if i==0:\n",
    "                gconv_layer = GCNConv(self.dim_pre_MLP, self.dim_graphLin)\n",
    "                self.graphLayers.append(gconv_layer)\n",
    "            else:\n",
    "                gconv_layer = GCNConv(self.dim_graphLin, self.dim_graphLin)\n",
    "                self.graphLayers.append(gconv_layer)\n",
    "        \n",
    "        \n",
    "        ### The graph convolutional layers are followed by post processing layers\n",
    "        ### Each MLP module has two linear layers, each followed by activation functions of ReLU\n",
    "        self.postGCLayers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        for i in range(self.num_post_layers):\n",
    "            if i==0 and i!=self.num_post_layers-1:\n",
    "                linear_layer = Linear(self.dim_graphLin, self.dim_post_MLP)\n",
    "                self.postGCLayers.append(linear_layer)\n",
    "                if self.training:\n",
    "                    self.dropout_layers.append(Dropout(self.dropout_prob))\n",
    "            \n",
    "            elif i==0 and i==self.num_post_layers-1:\n",
    "                linear_layer = Linear(self.dim_graphLin, 1)\n",
    "                self.postGCLayers.append(linear_layer)\n",
    "\n",
    "            elif i>0 and i<self.num_post_layers-1:\n",
    "                linear_layer = Linear(self.dim_post_MLP + self.dim_post_MLP, self.dim_post_MLP)\n",
    "                self.postGCLayers.append(linear_layer)\n",
    "                if self.training:\n",
    "                    self.dropout_layers.append(Dropout(self.dropout_prob))\n",
    "                \n",
    "            else:\n",
    "                linear_layer = Linear(self.dim_post_MLP + self.dim_post_MLP, 1)\n",
    "                self.postGCLayers.append(linear_layer)\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_weights, batch):\n",
    "        ## Pre-processing layers (MLP modules)\n",
    "        # print(batch)\n",
    "        # print(f\"The number of unique batches in the batch tensor is {len(torch.unique(batch))}\")\n",
    "        if self.num_pre_layers == 1:\n",
    "            x = self.relu(self.MLP[i](x))\n",
    "        else:\n",
    "            for i in range(self.num_pre_layers):\n",
    "                if i < self.num_pre_layers-1:\n",
    "                    # print(f\"The shape of the input at the start of the pre-processing layer{i} is {x.shape}\")\n",
    "                    x_out = self.relu(self.MLP[i](x))\n",
    "                    # print(f\"The shape of the input at the end of the pre-processing layer{i} is {x_out.shape}\")\n",
    "                    x = torch.cat([x_out,x], dim=-1)\n",
    "                    # print(f\"The shape of the input after concatenation at the end of the pre-processing layer{i} is {x.shape}\")\n",
    "                else:\n",
    "                    x = self.relu(self.MLP[i](x))\n",
    "                    # print(f\"The shape of the input at the end of the pre-processing layer{i} is {x.shape}\")\n",
    "\n",
    "        # print(f\"The shape of the input at the end of the pre-processing MLP layers is {x.shape}\")\n",
    "        # print(f\"The shape of the the weights fed to the model is {edge_weights.shape}\")\n",
    "        ## Node embeddings\n",
    "        for k in range(self.num_graph_layers):\n",
    "            x = self.graphLayers[k](x, edge_indices, edge_weights)\n",
    "            # print(f\"The shape of the embedding at the end of the graph layer {k} is {x.shape}\")\n",
    "            x = self.relu(x)\n",
    "\n",
    "        ## Graph-level pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        ### Post-processing and classification\n",
    "        if self.num_post_layers == 1:\n",
    "            x = self.sigmoid(self.postGCLayers[i](x))\n",
    "        else:\n",
    "            for i in range(self.num_post_layers):\n",
    "                if i < self.num_post_layers-1:\n",
    "                    x = self.postGCLayers[i](x)\n",
    "                    if self.training:\n",
    "                        x = self.dropout_layers[i](x)\n",
    "                    x_out = self.relu(x)\n",
    "                    x = torch.cat([x_out,x], dim=-1)\n",
    "\n",
    "                else:\n",
    "                    x = self.sigmoid(self.postGCLayers[i](x))\n",
    "            # print(f\"The shape of the input at the end of the post-processing MLP layer {i} is {h_concat.shape}\")\n",
    "\n",
    "        # print(f\"The shape of the input at the end of the post-processing MLP layers is {h_concat.shape}\")\n",
    "        # print(f\"The output is:{h_concat}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters to be tested\n",
    "training = True\n",
    "dim_pre_MLP = 32\n",
    "dim_post_MLP = 32\n",
    "dim_graphLin = 32\n",
    "num_pre_layers = 2\n",
    "num_post_layers = 2\n",
    "dropout_prob = 0.1\n",
    "num_graph_layers = 3\n",
    "patience_new = 15\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_pre_MLP : 32\n",
      "dim_post_MLP : 32\n",
      "dim_graphLin : 32\n",
      "num_pre_layers : 2\n",
      "num_post_layers : 2\n",
      "dropout_prob : 0.1\n",
      "num_graph_layers : 3\n",
      "patience : 15\n",
      "batch size : 32\n",
      "num neighbours : 70\n",
      "learning rate : 0.001\n"
     ]
    }
   ],
   "source": [
    "dict = {\"dim_pre_MLP\": dim_pre_MLP,\n",
    "        \"dim_post_MLP\": dim_post_MLP,\n",
    "        \"dim_graphLin\": dim_graphLin,\n",
    "        \"num_pre_layers\": num_pre_layers,\n",
    "        \"num_post_layers\": num_post_layers,\n",
    "        \"dropout_prob\": dropout_prob,\n",
    "        \"num_graph_layers\": num_graph_layers,\n",
    "        \"patience\": patience_new,\n",
    "        \"batch size\": batch_size,\n",
    "        \"num neighbours\": num_neighbours,\n",
    "        \"learning rate\": learning_rate}\n",
    "\n",
    "for key, val in dict.items():\n",
    "    print(f\"{key} : {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File save names to be used for the saving of metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_str = f\"GraphClassification_Originallength_GCN_pre_MLP_dim_{dim_pre_MLP}\"\n",
    "curves_filename = os.getcwd()+'/training_history_'+'/'+ file_str +'.jpg'\n",
    "model_filename = os.getcwd()+'/model_saves_'+'/'+ file_str +'.pt'\n",
    "accuracy_filename = os.getcwd()+'/accuracy_saves_'+'/'+ file_str+'.txt'\n",
    "time_filename = os.getcwd()+'/time_saves_'+'/'+ file_str+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    # print(f\"Output:{preds}\")\n",
    "    # print(f\"Labels:{labels}\")\n",
    "    predicted_classes = (preds > 0.5).float()\n",
    "    # print(f\"Predicted classes:{predicted_classes}\")\n",
    "    num_correct = (predicted_classes == labels).float().sum()\n",
    "    # print(f\"Number of correctly predicted results: {num_correct}\")\n",
    "    # print(f\"Total number of items: {labels.shape[0]}\")\n",
    "    accuracy = num_correct / labels.shape[0]\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def trainVal(model, dataloader):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay= 0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5)\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "    ##Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience = patience_new\n",
    "    counter = 0\n",
    "    best_model_path = model_filename\n",
    "\n",
    "    ## Keep track of the losses and accuracies over epochs\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        model.train() ## Training mode\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            edge_indices = batch.edge_index.to(torch.int64)\n",
    "            node_features = (torch.tensor(batch.x)).reshape(len(batch.x) * len(batch.x[0]),1).float() \n",
    "            edge_weights = (torch.tensor(batch.edge_weight)).reshape(len(batch.x) * len(batch.edge_weight[0])).float()\n",
    "            labels = batch.y.reshape(len(batch.x) ,1).float()\n",
    "            batches = batch.batch\n",
    "            num_unique_batches = len(torch.unique(batches))\n",
    "            batches %= num_unique_batches\n",
    "\n",
    "            ## Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(node_features, edge_indices, edge_weights, batches)\n",
    "            # print(f\"Output shape is:{output.shape}\")\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            ## Backward propagation and optimization \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            ## Keep track of the losses and accuracies\n",
    "            train_loss += loss.item()\n",
    "            train_acc += compute_accuracy(output, labels)\n",
    "\n",
    "        # print(f\"Dataloader length:{len(dataloader)}\")\n",
    "        # print(f\"Train loss: {train_loss}\")\n",
    "        # print(f\"Train accuracy: {train_acc}\")\n",
    "        \n",
    "        epoch_loss = train_loss / len(dataloader)\n",
    "        epoch_accuracy = train_acc / len(dataloader)\n",
    "\n",
    "        training_losses.append(epoch_loss)\n",
    "        training_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        ## validation\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        \n",
    "        model.eval()  ## evaluation state\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                edge_indices = batch.edge_index.to(torch.int64)\n",
    "                node_features = (torch.tensor(batch.x)).reshape(len(batch.x) * len(batch.x[0]),1).float() \n",
    "                edge_weights = (torch.tensor(batch.edge_weight)).reshape(len(batch.x) * len(batch.edge_weight[0])).float()\n",
    "                labels = batch.y.reshape(len(batch.x) ,1).float()\n",
    "                batches = batch.batch\n",
    "                num_unique_batches = len(torch.unique(batches))\n",
    "                batches %= num_unique_batches\n",
    "\n",
    "                # edge_indices = data[0]\n",
    "                # node_features = data[1]\n",
    "                # edge_weights = data[2]\n",
    "                # labels = data[3]\n",
    "                # batches = data[4]\n",
    "        \n",
    "                output = model(node_features, edge_indices, edge_weights, batches)\n",
    "                val_loss += criterion(output, labels).item()\n",
    "                val_acc += compute_accuracy(output, labels)\n",
    "\n",
    "        validation_loss = val_loss/len(val_dataloader)\n",
    "        validation_acc = val_acc/len(val_dataloader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracies.append(validation_acc)\n",
    "\n",
    "        ## print metrics every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {epoch_loss:.2f}'\n",
    "              f'| Train Acc: {epoch_accuracy*100:4.3f}% '\n",
    "              f'| Val Loss: {validation_loss:.2f} '\n",
    "              f'| Val Acc: {validation_acc*100: 4.3f}%')\n",
    "\n",
    "        ### Check for the validation loss improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            # print(f'Saving the model with val loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            counter +=1\n",
    "\n",
    "        ### Early stopping condition\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    ## Put the losses and accuracies in a dictionary which can be returned\n",
    "    losses = {\"Train loss\": training_losses,\n",
    "              \"Train accuracy\": training_accuracies,\n",
    "              \"Validation loss\": validation_losses,\n",
    "              \"Validation accuracy\": validation_accuracies}\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        edge_indices = batch.edge_index.to(torch.int64)\n",
    "        node_features = (torch.tensor(batch.x)).reshape(len(batch.x) * len(batch.x[0]),1).float() \n",
    "        edge_weights = (torch.tensor(batch.edge_weight)).reshape(len(batch.x) * len(batch.edge_weight[0])).float()\n",
    "        labels = batch.y.reshape(len(batch.x) ,1).float()\n",
    "        batches = batch.batch\n",
    "        num_unique_batches = len(torch.unique(batches))\n",
    "        batches %= num_unique_batches\n",
    "        \n",
    "        output = model(node_features, edge_indices, edge_weights, batches)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "        test_acc += compute_accuracy(output, labels)\n",
    "\n",
    "    testing_loss = test_loss / len(test_dataloader)\n",
    "    testing_acc = test_acc / len(test_dataloader)\n",
    "\n",
    "    return testing_loss, testing_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# ### Training and evaluation of the model\n",
    "# model = GraphConvolutionalNetwork(dim_pre_MLP, dim_post_MLP, dim_graphLin, num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training)\n",
    "# train_val_metrics = trainVal(model, train_dataloader)\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Elapsed time is {elapsed_time / 60:3.2f} minutes.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using GCN with skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "### Training and evaluation of the model\n",
    "model = GraphConvolutionNetworkWithSkipConnections(dim_pre_MLP, dim_post_MLP, dim_graphLin, \n",
    "                                                   num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training)\n",
    "train_val_metrics = trainVal(model, train_dataloader)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time is {elapsed_time / 60:3.2f} minutes.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = np.array([round(elapsed_time, 4)])\n",
    "np.savetxt(time_filename, elapsed_time, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the model training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"]=\"serif\"\n",
    "epochs = np.arange(0, len(train_val_metrics[\"Train loss\"]))\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=[8,3], dpi=300)\n",
    "ax[0].plot(epochs, train_val_metrics[\"Train loss\"], 'red', linestyle = '-', linewidth=1.0, marker = 's', mfc = 'k', markersize = 3, label = 'Training loss')\n",
    "ax[0].plot(epochs, train_val_metrics[\"Validation loss\"], 'blue', linestyle = '-', linewidth=1.0, marker = 's', mfc = 'k', markersize = 3, label = 'Validation loss')\n",
    "ax[0].set_title('Loss curves')\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Losses\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epochs, train_val_metrics[\"Train accuracy\"], 'red', linestyle = '-', linewidth=1.0, marker = 's', mfc = 'k', markersize = 3, label = 'Training accuracy')\n",
    "ax[1].plot(epochs, train_val_metrics[\"Validation accuracy\"], 'blue', linestyle = '-', linewidth=1.0, marker = 's', mfc = 'k', markersize = 3, label = 'Validation accuracy')\n",
    "ax[1].set_title('Accuracy curves')\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(curves_filename,dpi=199)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test data and calculating the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samar\\AppData\\Local\\Temp\\ipykernel_5556\\1050051353.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6930\n",
      "Test accuracy: 0.4997\n"
     ]
    }
   ],
   "source": [
    "# model = GraphConvolutionalNetwork(dim_pre_MLP, dim_post_MLP, dim_graphLin, num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training=False)\n",
    "model = GraphConvolutionNetworkWithSkipConnections(dim_pre_MLP, dim_post_MLP, dim_graphLin, \n",
    "                                                   # num_pre_layers, num_post_layers, dropout_prob, num_graph_layers, training=False)\n",
    "model.load_state_dict(torch.load(model_filename))\n",
    "model.eval()\n",
    "\n",
    "test_metrics = test(model)\n",
    "formatted_accuracy = \"{:.4f}\".format(test_metrics[1])\n",
    "formatted_loss = \"{:.4f}\".format(test_metrics[0])\n",
    "\n",
    "print(f\"Test loss: {formatted_loss}\")\n",
    "print(f\"Test accuracy: {formatted_accuracy}\")\n",
    "\n",
    "test_loss = test_metrics[0]\n",
    "test_accuracy = np.array([round(test_metrics[1], 4)])\n",
    "\n",
    "np.savetxt(accuracy_filename, test_accuracy, fmt='%.4f') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
