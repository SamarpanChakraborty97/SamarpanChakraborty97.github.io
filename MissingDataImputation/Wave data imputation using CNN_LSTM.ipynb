{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended for imputation of missing values in time series datasets obtained from wave buoys using attention based combination of Long-Short Term Memory (LSTM) and Convolutional Neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The data is obtained from the [CDIP buoys](https://cdip.ucsd.edu/m/stn_table/) website, where wave surface elevation data from different surface buoys in the form of netcdf files are obtained by using a custom $\\text{MATLAB}$ script. The netcdf files contain historic as well as real-time data of different measured wave variables. \n",
    "\n",
    "- **The objective was to predict possible sudden high-amplitude waves (wave annomalies (better known as *rogue waves*) that might be present in the missing piece of data. These waves occur suddenly in the time scale of seconds, hence it made sense to utilize the records of an observed variable which has a high sampling rate.**\n",
    "- Keeping this in mind, the wave surface elevation data which has a sampling rate of 1.28*Hz* was used. Other buoy variables, either did not directly capture the wave magnitudes or had a much lower sampling rate (order of minutes).\n",
    "- Thus, it made sense to use surface wave elevation data for the wave data imputation exercise.\n",
    "- The data is acquired in the form of windows spanning from 20 to 25 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data pre-processing\n",
    "- The wave peaks and troughs are filtered, since they embody the extreme values in the time series.\n",
    "- These are then broken down to two sections with a specific duration of data missing between them. The objective is to impute *these peakls in the time series data*.   \n",
    "- The $\\text{MATLAB}$ script above employs a wave modelling equation to fit the obtained data peak values in the time windows. This involves the use of a *regularization parameter $\\alpha$* which penalizes the extreme values of wave magnitudes.\n",
    "- Along with this, number of Fourier components used for breaking down the wave data were also investigated to give the best fit.\n",
    "- The fit data was then broken down into individual time series datasets corresponding to the number of Fourier components. These qualify as the final processed datasets to be used for the ML and other quantitative modelling techniques.\n",
    "- To summarize, we had ***100 data windows*** and ***N = 33 components***. This resulted in **33 datasets corresponding to the portion preceding the missing data, 33 datasets corresponding to the portion following the missing data for each window**.\n",
    "- Finally, a lagged table was created for each of these components and **33 total datasets for 1 window** were created by combining the trailing and leading portions. A lag value of ***M=200*** was chosen after parametric studies. This corresponded to the case which resulted in the **lowest validation error** for a sample window.\n",
    "- Tools used: ***pandas***, ***Numpy***      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-based CNN+LSTM model architecture\n",
    "- The model architecture consists of three parts.\n",
    "- The initial time series dataset is fed to a **LSTM model**.\n",
    "- In parallel, a scalogram capturing the time-frequency relationship in the time series is also obtained through wavelet analysis.\n",
    "- This **scalogram is fed to a CNN model** to extract the different features inherent in the time-frequency relationship.\n",
    "- The **LSTM is responsible for capturing the long-term trends in the time series**. The **CNN**, on the other hand, **is reponsible for capturing the localized features**, through proper selection of the kernel sizes and strides in the CNN. 3 layers of convolution features have been used.\n",
    "- In the second part of the model, the output from the last layer of the CNN model is combined with the output from the last hidden layer of the LSTM model. The **combined features are then fed to an attention mechanism using a Multi-Layer Perceptron (MLP) network**.\n",
    "- In the third part, the **output from this attention process is combined with the output from the last hidden layer of the LSTM and fed to a MLP for the final output at the next time step**. This process is continued iteratively till the intended imputation duration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn_lstm](cnn_lstm.jpg)\n",
    "**Architecture of the attentive CNN + LSTM network used for the imputation of missing entries in the present studies. The input sequence, on one hand is fed to a LSTM. Along with this, the time series sequence is used to create a scalogram, which is fed to a CNN model. The outputs from these models are utilized to get attention scores of the CNN features, which are then fed alongside the outputs of the last LSTM hidden layer to an MLP model to get the final prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+LSTM Model Workflow \n",
    "- The model workflow comprises of the following processes:\n",
    "    1. Reading the pre-processed data using **pandas** and **Numpy**.\n",
    "    3. Generating the dataset using a lagged table to be used as input for the LSTM model. \n",
    "    4. Using wavelet analysis to generate scalogram plots to be used as inputs for the CNN model. \n",
    "    5. Standardizing the features and creating the train, validation and test datasets.\n",
    "    6. Training the above model architecture using custom optimizer class on the data using **PyTorch**.\n",
    "    7. Validating the trained models through a early stopping mechanism\n",
    "    8. Storing the trained model.\n",
    "    9. Evaluating the test data on the trained model.\n",
    "- The workflow thus discussed constitutes **the imputed values for 1 of the 33 components**. A for-loop is used for imputation of missing values for all the 33 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the datasets for feeding into the LSTM and the CNN+LSTM models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "transform = transforms.ToTensor()\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import datetime as datetime\n",
    "from timeit import default_timer as timer\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "transform2 = transforms.Resize((54,55))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "for i in range(1,3):\n",
    "    data_pre = pd.read_csv(f\"Slow_amp_pre_{i}.csv\", header=None)\n",
    "    data_post = pd.read_csv(f\"Slow_amp_post_{i}.csv\", header=None)\n",
    "    data_whole = pd.read_csv(f\"Slow_amp_whole_{i}.csv\", header=None)\n",
    "    \n",
    "    n_rows = data_pre.shape[0]\n",
    "    n_cols = data_whole.shape[1] - (data_pre.shape[1] + data_post.shape[1])\n",
    "\n",
    "    data_miss = pd.DataFrame(np.zeros([n_rows, n_cols])*np.nan)\n",
    "    \n",
    "    data_pre_vals = data_pre[:].values\n",
    "    data_post_vals = data_post[:].values\n",
    "    data_whole_vals = data_whole[:].values\n",
    "    \n",
    "    data_test = scaler.fit_transform(data_whole_vals.reshape(-1,1)).reshape(data_whole_vals.shape[0],data_whole_vals.shape[1])\n",
    "    missing_len = data_miss.shape[1]\n",
    "    \n",
    "    #Predictions_pre = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "    #Values = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "\n",
    "    dummy_data = pd.concat([data_pre, data_post], axis=1,ignore_index=True)\n",
    "    data = scaler.fit_transform(dummy_data[:].values.reshape(-1,1)).reshape(dummy_data.shape[0],dummy_data.shape[1])\n",
    "    pre_data_scaled = data[:,:data_pre.shape[1]]\n",
    "    pre_shape = pre_data_scaled.shape\n",
    "    #data = data[:3,-254:]\n",
    "    \n",
    "    input_len = 200\n",
    "    output_len = 1  \n",
    "    input_dim = input_len\n",
    "    output_dim = output_len\n",
    "    tuple_shape = (53, 54)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    L1 = data.shape[1] - input_len\n",
    "    train_len = int(0.7 * L1)\n",
    "    val_len = L1 - train_len\n",
    "    \n",
    "    X_train_pre_lstm_tensor = torch.zeros(len(data), train_len, input_len)\n",
    "    Y_train_pre_tensor = torch.zeros(len(data), train_len, output_len)\n",
    "    X_train_pre_CNN_tensor = torch.zeros(len(data), train_len, 3 , 53, 54)\n",
    "    \n",
    "    \n",
    "    X_val_pre_lstm_tensor = torch.zeros(len(data), val_len, input_len)\n",
    "    Y_val_pre_tensor = torch.zeros(len(data), val_len, output_len)\n",
    "    X_val_pre_CNN_tensor = torch.zeros(len(data), val_len, 3 , 53, 54)\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        X_pre = np.zeros([L1, input_len])\n",
    "        Y_pre = np.zeros([L1, output_len])\n",
    "\n",
    "        for k in range(L1):\n",
    "            X_pre[k,:] = data[j,k:k+input_len]\n",
    "            Y_pre[k,:] = data[j,k+input_len:k+input_len+output_len]\n",
    "\n",
    "        Train_X_pre = X_pre[:train_len]\n",
    "        Train_Y_pre = Y_pre[:train_len]\n",
    "        \n",
    "        Val_X_pre = X_pre[train_len:]        \n",
    "        Val_Y_pre = Y_pre[train_len:]\n",
    "    \n",
    "        X_train_pre_lstm_tensor[j] = torch.Tensor(Train_X_pre.copy())\n",
    "        Y_train_pre_tensor[j] = torch.Tensor(Train_Y_pre.copy())\n",
    "    \n",
    "        X_val_pre_lstm_tensor[j] = torch.Tensor(Val_X_pre.copy())\n",
    "        Y_val_pre_tensor[j] = torch.Tensor(Val_Y_pre.copy())\n",
    "        \n",
    "        for m in range(train_len):            \n",
    "            time = (1/1.28)*np.arange(0, len(Train_X_pre[m]))\n",
    "            signal = Train_X_pre[m]\n",
    "            scales = np.arange(1, 256)\n",
    "            dt = time[1] - time[0]\n",
    "            waveletname = 'cmor' \n",
    "            cmap = plt.cm.jet\n",
    "            [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "            power = (abs(coefficients)) ** 2    \n",
    "            p_h = 0.05\n",
    "            p_l = 0.0\n",
    "            levs = np.arange(p_l,p_h,(p_h - p_l)/100)    \n",
    "            fig = plt.figure(figsize=(0.7,0.7))\n",
    "            im = plt.contourf(time, frequencies, power, cmap = cmap, vmin=p_l, vmax=p_h, levels=levs, extend='both')\n",
    "            plt.axis('off')\n",
    "            plt.xlim([20, 150])\n",
    "            plt.ylim([0.005, 0.04])\n",
    "            plt.savefig(f'Sample{i}.jpeg', bbox_inches='tight',pad_inches = 0)\n",
    "            plt.close()\n",
    "            #print(f'm:{m}')\n",
    "        \n",
    "            image = Image.open(f'Sample{i}.jpeg')\n",
    "            tensor = transform(image)\n",
    "            X_train_pre_CNN_tensor[j,m] = tensor\n",
    "\n",
    "        for m in range(val_len) :  \n",
    "            time = (1/1.28)*np.arange(0, len(Val_X_pre[m]))\n",
    "            signal = Val_X_pre[m]\n",
    "            scales = np.arange(1, 256)\n",
    "            dt = time[1] - time[0]\n",
    "            waveletname = 'cmor' \n",
    "            cmap = plt.cm.jet\n",
    "            [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "            power = (abs(coefficients)) ** 2   \n",
    "            p_h = 0.05\n",
    "            p_l = 0.0\n",
    "            levs = np.arange(p_l,p_h,(p_h - p_l)/100)    \n",
    "            fig = plt.figure(figsize=(0.7,0.7))\n",
    "            im = plt.contourf(time, frequencies, power, cmap = cmap, vmin=p_l, vmax=p_h, levels=levs, extend='both')\n",
    "            plt.axis('off')\n",
    "            plt.xlim([20, 150])\n",
    "            plt.ylim([0.005, 0.04])\n",
    "            plt.savefig(f'SampleVal{i}.jpeg', bbox_inches='tight',pad_inches = 0)\n",
    "            plt.close()\n",
    "        \n",
    "            image = Image.open(f'SampleVal{i}.jpeg')\n",
    "            tensor = transform(image)\n",
    "            X_val_pre_CNN_tensor[j,m] = tensor\n",
    "            \n",
    "        #print(f'j:{j}')\n",
    "            \n",
    "    torch.save(X_train_pre_lstm_tensor, f'X_Train_LSTM_Tensor_{i}.pt')\n",
    "    torch.save(Y_train_pre_tensor, f'Y_Train_Tensor_{i}.pt')\n",
    "    torch.save(X_train_pre_CNN_tensor, f'X_CNN_LSTM_Train_Tensor_{i}.pt')\n",
    "    \n",
    "    torch.save(X_val_pre_lstm_tensor, f'X_Val_LSTM_Tensor_{i}.pt')\n",
    "    torch.save(Y_val_pre_tensor, f'Y_Val_Tensor_{i}.pt')\n",
    "    torch.save(X_val_pre_CNN_tensor, f'X_CNN_LSTM_Val_Tensor_{i}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import datetime as datetime\n",
    "from timeit import default_timer as timer\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms for creating images to PyTorch tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "transform = transforms.ToTensor()\n",
    "transform2 = transforms.Resize((53,54))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN+LSTM model class encompassing the LSTM, CNN+LSTM and the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Module(nn.Module):\n",
    "    def __init__(self, oc1, s_conv, ks, dil1, dil2, ts, i_dim, h_dim, l_dim, d_prob, mlp_hdim1, mlp_odim, a_net_feature):\n",
    "        self.oc1 = oc1\n",
    "        self.s_conv = s_conv\n",
    "        self.ks = ks\n",
    "        self.ts = ts\n",
    "        self.dil1 = dil1\n",
    "        self.dil2 = dil2\n",
    "        \n",
    "        self.i_dim = i_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.l_dim = l_dim\n",
    "        self.d_prob = d_prob\n",
    "        self.mlp_hdim1 = mlp_hdim1\n",
    "        self.mlp_odim = mlp_odim\n",
    "        self.a_net_feature = a_net_feature\n",
    "\n",
    "        super(CNN_LSTM_Module, self).__init__()\n",
    "\n",
    "        '''Convolution part of the model'''\n",
    "        \n",
    "        self.convD1_1 = Conv2d(in_channels=3, out_channels=oc1, dilation = dil1, kernel_size=ks)\n",
    "        self.convD1_2 = Conv2d(in_channels=oc1, out_channels=oc1, dilation = dil1, kernel_size=ks)\n",
    "        \n",
    "        self.convD2_1 = Conv2d(in_channels=3, out_channels=oc1, dilation = dil2, kernel_size=ks)\n",
    "        self.convD2_2 = Conv2d(in_channels=oc1, out_channels=oc1, dilation = dil2, kernel_size=ks)\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "        \n",
    "        ### DILATION VAL 1 ###\n",
    "        \n",
    "        ### initialize first set of CONV => RELU => layers ###\n",
    "        size1_1 = math.floor((ts[0] - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        size1_2 = math.floor((ts[1] - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        \n",
    "        ### initialize second set of CONV => RELU => layers ###\n",
    "        size1_1 = math.floor((size1_1 - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        size1_2 = math.floor((size1_2 - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        \n",
    "        ### initialize third set of CONV => RELU => layers ###\n",
    "        size1_1 = math.floor((size1_1 - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        size1_2 = math.floor((size1_2 - dil1 * (ks-1) - 1)/s_conv)+1\n",
    "        \n",
    "        \"\"\"LSTM part of the model\"\"\"\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(i_dim, h_dim, l_dim, batch_first=True, dropout=0.4)\n",
    "\n",
    "        \"\"\"Attention module of the network\"\"\"\n",
    "        out_features_Layer1 = a_net_feature\n",
    "        \n",
    "        D1_in_features_Layer1 = (size1_1 * size1_2) + (h_dim * l_dim)\n",
    "        self.attentionD1 = Linear(D1_in_features_Layer1, out_features_Layer1)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        in_features_Layer2 = self.a_net_feature\n",
    "        out_features_Layer2 = 1\n",
    "        self.attention2 = Linear(in_features_Layer2, out_features_Layer2)\n",
    "\n",
    "        self.smax = nn.Softmax(dim=1)\n",
    "\n",
    "        \"\"\"Fusion and Predictions using Multi Layer Perceptron\"\"\"\n",
    "        \n",
    "        #fusion_input_dim = (size1_1 * size1_2) + (size2_1 * size2_2) + (self.h_dim * self.l_dim)\n",
    "        fusion_input_dim = (size1_1 * size1_2) + (self.h_dim * self.l_dim)\n",
    "        fusion_hidden_dim1 = math.floor(fusion_input_dim/2)\n",
    "        fusion_output_dim = self.mlp_odim\n",
    "\n",
    "        self.fc1 = nn.Linear(fusion_input_dim, fusion_hidden_dim1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(self.d_prob)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = nn.Linear(fusion_hidden_dim1, fusion_output_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        '''CNN 2D convolution using scaleograms'''\n",
    "        \n",
    "        #### 1st dilation variant ####\n",
    "        \n",
    "        xD1 = self.convD1_1(x1)\n",
    "        xD1 = self.relu(xD1)\n",
    "\n",
    "        xD1 = self.convD1_2(xD1)\n",
    "        xD1 = self.relu(xD1)\n",
    "\n",
    "        xD1 = self.convD1_2(xD1)\n",
    "        xD1 = self.relu(xD1)\n",
    "\n",
    "        xD1_inter = xD1.reshape(x2.size(0), -1, xD1.shape[2]*xD1.shape[3])\n",
    "\n",
    "        '''Hidden state prediction using LSTM for the global trend'''\n",
    "        h0 = torch.zeros(self.l_dim, x2.size(0), self.h_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.l_dim, x2.size(0), self.h_dim).requires_grad_().to(device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x2, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        #print(\"The size of the hidden LSTM output directly has the shape of {}\".format(hn.size()))\n",
    "        \n",
    "        #a = torch.Tensor([[]]).to(device)\n",
    "        #for i in range(hn.shape[0]):\n",
    "        #    a = torch.cat((a,hn[i]),1)\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), 1)\n",
    "        \n",
    "        #hn = hn.reshape([x2.size(0),-1])\n",
    "        #print(\"The size of the hidden LSTM output has the shape of {}\".format(hn.size()))\n",
    "        \n",
    "        hn_rec1 = hn.reshape(hn.shape[0],1,hn.shape[1])\n",
    "        for l in range(1, xD1_inter.shape[1]):\n",
    "            hn_rec1 = torch.cat((hn_rec1, hn.reshape(hn.shape[0],1,hn.shape[1])),1)\n",
    "        \n",
    "        #hn_rec2 = hn.reshape(hn.shape[0],1,hn.shape[1])\n",
    "        #for l in range(1, xD2_inter.shape[1]):\n",
    "        #    hn_rec2 = torch.cat((hn_rec2, hn.reshape(hn.shape[0],1,hn.shape[1])),1)\n",
    "\n",
    "        #print(\"The size of the hidden LSTM output after concatenation has the shape of {}\".format(hn_rec.size()))\n",
    "\n",
    "        #hn_rec = hn_rec.reshape([-1,x1_inter.size(1),hn.size(1)])\n",
    "        #print(\"The size of the reconstructed LSTM hidden layer has the shape of {}\".format(hn_rec.size()))\n",
    "        \n",
    "        '''Attention module using the hidden states and the CNN module'''\n",
    "        xd1 = torch.concat((xD1_inter,hn_rec1),dim=2)\n",
    "        #xd2 = torch.concat((xD2_inter,hn_rec2),dim=2)\n",
    "        \n",
    "        #print(\"The size of the augmented input attention layer has the shape of {}\".format(x.size()))\n",
    "        \n",
    "        xdil1 = self.attentionD1(xd1)\n",
    "        #xdil2 = self.attentionD2(xd2)\n",
    "        \n",
    "        #print(\"The size of the input after the first attention layer has the shape of {}\".format(x.size()))\n",
    "        \n",
    "        xdil1 = self.tanh(xdil1)\n",
    "        #xdil2 = self.tanh(xdil2)\n",
    "        \n",
    "        xdil1 = self.attention2(xdil1)\n",
    "        #xdil2 = self.attention2(xdil2)\n",
    "        \n",
    "        #print(\"The size of the input after the second attention layer has the shape of {}\".format(x.size()))\n",
    "        \n",
    "        xdil1 = self.smax(xdil1)\n",
    "        #xdil2 = self.smax(xdil2)\n",
    "        \n",
    "        #print(\"The size of the input after the softmax layer has the shape of {}\".format(x.size()))\n",
    "        \n",
    "        #print(torch.transpose(x1_inter,1,2).shape)\n",
    "        \n",
    "        xdil1 = torch.bmm(torch.transpose(xD1_inter,1,2),xdil1).to(device)\n",
    "        #xdil2 = torch.bmm(torch.transpose(xD2_inter,1,2),xdil2).to(device)\n",
    "        \n",
    "        #print(\"The size of the input after the weighted sum has the shape of {}\".format(x.size()))\n",
    "        \n",
    "        xdil1 = xdil1.reshape([x2.size(0),-1])\n",
    "        #xdil2 = xdil2.reshape([x2.size(0),-1])\n",
    "        \n",
    "        #print(\"The size of the reshaped input layer before MLP has the shape of {}\".format(x.size()))\n",
    "\n",
    "        '''Fusion and prediction using MLP'''\n",
    "        #x_MLP = torch.concat((xdil1, xdil2, hn),dim=1)\n",
    "        x_MLP = torch.concat((xdil1, hn),dim=1)\n",
    "        \n",
    "        #print(\"The size of the augmented input MLP layer` has the shape of {}\".format(x_MLP.size()))\n",
    "        \n",
    "        x_MLP = self.fc1(x_MLP)\n",
    "        \n",
    "        #print(\"The size of the input after the first hidden MLP layer has the shape of {}\".format(x_MLP.size()))\n",
    "        \n",
    "        x_MLP = self.relu(x_MLP)\n",
    "        \n",
    "        x_MLP = self.dropout(x_MLP)\n",
    "        \n",
    "        x_MLP = self.fc2(x_MLP)\n",
    "        \n",
    "        output = x_MLP\n",
    "        \n",
    "        #print(\"The size of the output after the MLP layer has the shape of {}\".format(output.size()))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom optimizer class to include the training, validation and loss plotting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer, patience, min_delta = 1e-5):\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.counter = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.patience = patience\n",
    "        \n",
    "    def train_step(self,x1,x2,y):\n",
    "        self.model.train()\n",
    "\n",
    "        yhat = self.model(x1,x2)\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def earlyStop(self, validation_loss):\n",
    "        if validation_loss < (self.min_validation_loss - self.min_delta):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "        elif validation_loss >= (self.min_validation_loss - self.min_delta):\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size, n_epochs, mode, n_features, output_dim):\n",
    "\n",
    "        model_path = f'cnn2d+lstm.pt'\n",
    "        break_out_flag = False\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x1_batch, x2_batch, y_batch in train_loader:\n",
    "                x2_batch = x2_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                x1_batch = x1_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                yhat = self.model(x1_batch, x2_batch)\n",
    "                loss = self.train_step(x1_batch, x2_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x1_val, x2_val, y_val in val_loader:\n",
    "                    x2_val = x2_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    x1_val = x1_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x1_val, x2_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "                if self.earlyStop(validation_loss):\n",
    "                    break_out_flag = True\n",
    "                    break               \n",
    "            \n",
    "            if break_out_flag:\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "                break\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 25 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "        #torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        \n",
    "    def evaluate(self, x, test, training_len, output_len, missing_len, sample):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for j in range(len(test)):\n",
    "                val = test[j].to(device).cpu()\n",
    "                values.append(val.detach().numpy())\n",
    "            \n",
    "            for i in range(missing_len):\n",
    "                x = x.to(device)\n",
    "                self.model.eval()\n",
    "                #x_test = x.view([1, -1, training_len]).to(device)\n",
    "                x_lstm_test = x.view([1, -1, training_len]).to(device)\n",
    "\n",
    "                x_a = x.cpu()\n",
    "                x_arr = np.array(x_a)\n",
    "                x_arr = x_arr.reshape(len(x_arr))\n",
    "                #print(x_arr.shape)\n",
    "\n",
    "                time =  time = (1/1.28)*np.arange(0, len(x_arr))\n",
    "                signal = x_arr\n",
    "                scales = np.arange(1, 256)\n",
    "\n",
    "                dt = time[1] - time[0]\n",
    "                waveletname = 'cmor'\n",
    "                cmap = plt.cm.jet\n",
    "                [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "                power = (abs(coefficients)) ** 2\n",
    "\n",
    "                p_h = 0.05\n",
    "                p_l = 0.0\n",
    "                levs = np.arange(p_l,p_h,(p_h - p_l)/100)    \n",
    "                fig = plt.figure(figsize=(0.7,0.7))\n",
    "                im = plt.contourf(time, frequencies, power, cmap = cmap, vmin=p_l, vmax=p_h, levels=levs, extend='both')\n",
    "                plt.axis('off')\n",
    "                plt.xlim([20, 150])\n",
    "                plt.ylim([0.005, 0.04])\n",
    "                plt.savefig(f'SamplePred{sample}.jpeg', bbox_inches='tight',pad_inches = 0)\n",
    "                plt.close()\n",
    "\n",
    "                image = Image.open(f'SamplePred{sample}.jpeg')\n",
    "\n",
    "                resized_image = transform2(image)\n",
    "                tensor = transform(resized_image)\n",
    "                \n",
    "                tensor = tensor.unsqueeze(0).to(device)\n",
    "                #print(tensor.size())\n",
    "\n",
    "                yhat = self.model(tensor, x_lstm_test)\n",
    "                yint = torch.reshape(yhat,(output_len,1))\n",
    "                \n",
    "                y_int = yint.to(device).cpu()\n",
    "                \n",
    "                predictions.append(y_int[-1].detach().numpy())\n",
    "                x=torch.reshape(x,(training_len,1))\n",
    "                x = torch.cat((x,yint[-1].reshape(1,1)),0)\n",
    "                x = x[-training_len:]\n",
    "            \n",
    "        preds =  torch.reshape(torch.Tensor(predictions),(-1,1))\n",
    "        \n",
    "        return np.asarray(values), np.asarray(preds) \n",
    "    \n",
    "    def evaluate2(self, x, test, training_len, missing_len):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for j in range(len(test)):\n",
    "                val = test[j].to(device).cpu()\n",
    "                values.append(val.detach().numpy())\n",
    "            \n",
    "            num = len(test) % missing_len\n",
    "            print(num)\n",
    "            if (num == 0):    \n",
    "                for i in range(math.floor(len(test)/missing_len)):\n",
    "                    x = x.to(device)\n",
    "                    self.model.eval()\n",
    "                    #x_test = x.view([1, -1, training_len]).to(device)\n",
    "                    x_lstm_test = x.view([1, -1, training_len]).to(device)\n",
    "\n",
    "                    x_a = x.cpu()\n",
    "                    x_arr = np.array(x_a)\n",
    "                    x_arr = x_arr.reshape(len(x_arr))\n",
    "                    #print(x_arr.shape)\n",
    "\n",
    "                    time =  time = (1/48)*np.arange(0, len(x_arr))\n",
    "                    signal = x_arr\n",
    "                    scales = np.arange(1, 512)\n",
    "\n",
    "                    dt = time[1] - time[0]\n",
    "                    waveletname = 'cmor'\n",
    "                    cmap = plt.cm.jet\n",
    "                    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "                    power = (abs(coefficients)) ** 2\n",
    "\n",
    "                    p_h = 0.1\n",
    "                    p_l = 0.0\n",
    "\n",
    "                    levs = np.arange(p_l,p_h,(p_h - p_l)/100)\n",
    "\n",
    "                    fig = plt.figure(figsize=(1,1))\n",
    "                    im = plt.contourf(time, frequencies, power, cmap = cmap, vmin=p_l, vmax=p_h, levels=levs, extend='both')\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(f'Sample2.jpeg', bbox_inches='tight',pad_inches = 0)\n",
    "                    plt.close()\n",
    "\n",
    "                    image = Image.open(f'Sample2.jpeg')\n",
    "\n",
    "                    resized_image = transform2(image)\n",
    "                    tensor = transform(resized_image)\n",
    "                \n",
    "                    tensor = tensor.unsqueeze(0).to(device)\n",
    "                    #print(tensor.size())\n",
    "\n",
    "                    yhat = self.model(tensor, x_lstm_test)\n",
    "                    yint = torch.reshape(yhat,(missing_len,1))\n",
    "                \n",
    "                    y_int = yint.to(device).cpu()\n",
    "                \n",
    "                    predictions.append(y_int.detach().numpy())\n",
    "                    x=torch.reshape(x,(training_len,1))\n",
    "                    x = torch.cat((x,yint),0)\n",
    "                    x = x[-training_len:]\n",
    "                    \n",
    "            else:\n",
    "                for i in range(math.floor(len(test)/missing_len)+1):\n",
    "                    x = x.to(device)\n",
    "                    self.model.eval()\n",
    "                    #x_test = x.view([1, -1, training_len]).to(device)\n",
    "                    x_lstm_test = x.view([1, -1, training_len]).to(device)\n",
    "\n",
    "                    x_a = x.cpu()\n",
    "                    x_arr = np.array(x_a)\n",
    "                    x_arr = x_arr.reshape(len(x_arr))\n",
    "                    #print(x_arr.shape)\n",
    "\n",
    "                    time =  time = (1/48)*np.arange(0, len(x_arr))\n",
    "                    signal = x_arr\n",
    "                    scales = np.arange(1, 512)\n",
    "\n",
    "                    dt = time[1] - time[0]\n",
    "                    waveletname = 'cmor'\n",
    "                    cmap = plt.cm.jet\n",
    "                    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "                    power = (abs(coefficients)) ** 2\n",
    "\n",
    "                    p_h = 0.1\n",
    "                    p_l = 0.0\n",
    "\n",
    "                    levs = np.arange(p_l,p_h,(p_h - p_l)/100)\n",
    "\n",
    "                    fig = plt.figure(figsize=(1,1))\n",
    "                    im = plt.contourf(time, frequencies, power, cmap = cmap, vmin=p_l, vmax=p_h, levels=levs, extend='both')\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(f'Sample2.jpeg', bbox_inches='tight',pad_inches = 0)\n",
    "                    plt.close()\n",
    "\n",
    "                    image = Image.open(f'Sample2.jpeg')\n",
    "\n",
    "                    resized_image = transform2(image)\n",
    "                    tensor = transform(resized_image)\n",
    "                \n",
    "                    tensor = tensor.unsqueeze(0).to(device)\n",
    "                    #print(tensor.size())\n",
    "\n",
    "                    yhat = self.model(tensor, x_lstm_test)\n",
    "                    yint = torch.reshape(yhat,(missing_len,1))\n",
    "                \n",
    "                    y_int = yint.to(device).cpu()\n",
    "                \n",
    "                    predictions.append(y_int.detach().numpy())\n",
    "                    x=torch.reshape(x,(training_len,1))\n",
    "                    x = torch.cat((x,yint),0)\n",
    "                    x = x[-training_len:]\n",
    "                    \n",
    "        preds =  torch.reshape(torch.Tensor(predictions),(-1,1))\n",
    "        return np.asarray(values), np.asarray(preds)\n",
    "\n",
    "\n",
    "    def plot_losses(self, output_len):\n",
    "        \"\"\"The method plots the calculated loss values for training and validation\n",
    "        \"\"\"\n",
    "        #np.savetxt(f\"CNN_LSTM_dilatedConcatenated_79_mean_train.out\", self.train_losses, fmt='%1.4e')\n",
    "        #np.savetxt(f\"CNN_LSTM_dilatedConcatenated_79_mean_val.out\", self.val_losses, fmt='%1.4e')\n",
    "        \n",
    "        #plt.figure(figsize=[10,8])\n",
    "        #plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        #plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        #plt.legend()\n",
    "        #plt.title(f\"Losses for dilated concatenation using mean reduction for 79 steps\")\n",
    "        #plt.grid()\n",
    "        #plt.show()\n",
    "        #plt.savefig(f'CNN_LSTM Losses comparisons for dilatedConcatenated_79_mean over epochs.png',dpi=300)\n",
    "        #plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CNN_LSTM_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, entries, labels):\n",
    "        self.X1 = images\n",
    "        self.X2 = entries\n",
    "        self.Y = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data1 = self.X1[idx,:]\n",
    "        data2 = self.X2[idx,:]\n",
    "\n",
    "        data_labels = self.Y[idx,:]\n",
    "\n",
    "        return (data1, data2, data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on the main data\n",
    "- The data is read from the created datasets here. \n",
    "- The diffent functions are called here. \n",
    "- The data is broken down into training, validation datasets after scaling them. \n",
    "- The missing data is taken as the test dataset. \n",
    "- The hyperparameters shown here are arrived at after extensive hyperparameter tuning *(convolutional features, hidden dimension, layer dimension, mlp hidden dimenison)*\n",
    "- Finally, the missing portion was imputed through iterative single step ahead predictions and the predictions and values stored for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 200\n",
    "output_len = 1\n",
    "    \n",
    "input_dim = input_len\n",
    "output_dim = output_len\n",
    "\n",
    "ks = 3\n",
    "hidden_dim = 200\n",
    "layer_dim = 2\n",
    "dropout_prob = 0.3\n",
    "a_net_feature = 32\n",
    "mlp_hiddendim1 = 100\n",
    "mlp_odim = output_len\n",
    "\n",
    "oc1 = 150\n",
    "\n",
    "dilation1 = 1\n",
    "dilation2 = 7\n",
    "\n",
    "tuple_shape = (53, 54)\n",
    "weight_decay = 1e-3\n",
    "    \n",
    "for i in range(1,3):\n",
    "    data_pre = pd.read_csv(f\"Slow_amp_pre_{i}.csv\", header=None)\n",
    "    data_post = pd.read_csv(f\"Slow_amp_post_{i}.csv\", header=None)\n",
    "    data_whole = pd.read_csv(f\"Slow_amp_whole_{i}.csv\", header=None)\n",
    "    \n",
    "    n_rows = data_pre.shape[0]\n",
    "    n_cols = data_whole.shape[1] - (data_pre.shape[1] + data_post.shape[1])\n",
    "\n",
    "    data_miss = pd.DataFrame(np.zeros([n_rows, n_cols])*np.nan)\n",
    "    \n",
    "    data_pre_vals = data_pre[:].values\n",
    "    data_post_vals = data_post[:].values\n",
    "    data_whole_vals = data_whole[:].values\n",
    "    \n",
    "    data_test = scaler.fit_transform(data_whole_vals.reshape(-1,1)).reshape(data_whole_vals.shape[0],data_whole_vals.shape[1])\n",
    "    missing_len = data_miss.shape[1]\n",
    "    \n",
    "    Predictions_pre = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "    Values = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "\n",
    "    dummy_data = pd.concat([data_pre, data_post], axis=1,ignore_index=True)\n",
    "    data = scaler.fit_transform(dummy_data[:].values.reshape(-1,1)).reshape(dummy_data.shape[0],dummy_data.shape[1])\n",
    "    pre_data_scaled = data[:,:data_pre.shape[1]]\n",
    "    pre_shape = pre_data_scaled.shape\n",
    "    #data = data[:3,-254:]\n",
    "    \n",
    "    input_len = 200\n",
    "    output_len = 1  \n",
    "    input_dim = input_len\n",
    "    output_dim = output_len\n",
    "    tuple_shape = (53, 54)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_lstm_train = torch.load( f'X_Train_LSTM_Tensor_{i}.pt')\n",
    "    y_train = torch.load(f'Y_Train_Tensor_{i}.pt')\n",
    "    x_cnn_train = torch.load( f'X_CNN_LSTM_Train_Tensor_{i}.pt')\n",
    "    \n",
    "    x_lstm_val = torch.load( f'X_Val_LSTM_Tensor_{i}.pt')\n",
    "    y_val = torch.load(f'Y_Val_Tensor_{i}.pt')\n",
    "    x_cnn_val = torch.load( f'X_CNN_LSTM_Val_Tensor_{i}.pt')\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        torch.manual_seed(2)\n",
    "    \n",
    "        train_pre_eta = CNN_LSTM_Dataset(x_cnn_train[j], x_lstm_train[j], y_train[j])\n",
    "        val_pre_eta = CNN_LSTM_Dataset(x_cnn_val[j], x_lstm_val[j], y_val[j])\n",
    "    \n",
    "        start = timer()\n",
    "    \n",
    "        model = CNN_LSTM_Module(oc1 = oc1, s_conv = 1, ks = ks, dil1 = dilation1, dil2 = dilation2, ts = tuple_shape, i_dim = input_dim, h_dim = hidden_dim, l_dim = layer_dim, d_prob = dropout_prob, mlp_hdim1 = mlp_hiddendim1, mlp_odim = output_len, a_net_feature = a_net_feature)\n",
    "        model = model.to(device)\n",
    "\n",
    "        batch_size = 32\n",
    "        n_epochs = 500\n",
    "\n",
    "        learning_rate = 1e-5\n",
    "        loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        #loss_fn = MeanCubeLoss()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "        Ftrain_loader = DataLoader(train_pre_eta, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        Fval_loader = DataLoader(val_pre_eta, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "        opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer, patience = 30)\n",
    "        opt.train(Ftrain_loader, Fval_loader, batch_size=batch_size, n_epochs=n_epochs, mode=i, n_features=input_dim, output_dim = output_dim)\n",
    "        opt.plot_losses(output_dim)\n",
    "            \n",
    "        end = timer()\n",
    "\n",
    "        dur = (end-start)/60\n",
    "        print(f'The total duration for the training is {dur} minutes')\n",
    "\n",
    "        X_Test = np.asarray(data_test[j,data_pre_vals.shape[1]-input_len:data_pre_vals.shape[1]])\n",
    "        Y_Test = np.asarray(data_test[j,data_pre_vals.shape[1]:data_pre_vals.shape[1]+missing_len])\n",
    "\n",
    "        Test_features = torch.Tensor(X_Test)\n",
    "        Test_targets = torch.Tensor(Y_Test)\n",
    "\n",
    "        #model = CNN_LSTM_Module(oc1 = oc1, s_conv = 1, ks = ks, dil1 = dilation1, dil2 = dilation2, ts = tuple_shape, i_dim = input_dim, h_dim = hidden_dim, l_dim = layer_dim, d_prob = dropout_prob, mlp_hdim1 = mlp_hiddendim1, mlp_odim = output_len, a_net_feature = a_net_feature)\n",
    "        #model = model.to(device)\n",
    "\n",
    "        #PATH = f'cnn2d+lstm.pt'\n",
    "        #model.load_state_dict(torch.load(PATH))\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "        bl1 = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer, patience = 50)\n",
    "        values, preds = bl1.evaluate(Test_features,Test_targets, input_len, output_len, missing_len,i)\n",
    "    \n",
    "        num = len(Test_targets) % output_len\n",
    "\n",
    "        if (num != 0):\n",
    "            preds = preds[:len(Test_targets)]\n",
    " \n",
    "        p = np.asarray(preds).reshape(missing_len)\n",
    "        Predictions_pre[j,:] = p\n",
    "        Values[j,:] = values\n",
    "    \n",
    "    Preds_rescaled = scaler.inverse_transform(Predictions_pre.reshape(-1,1)).reshape(Predictions_pre.shape[0],Predictions_pre.shape[1])\n",
    "    Vals_rescaled = scaler.inverse_transform(Values.reshape(-1,1)).reshape(Predictions_pre.shape[0],Predictions_pre.shape[1])\n",
    "    \n",
    "    np.savetxt(f\"Preds_pre_cnn+lstm_{i}.out\", Preds_rescaled)\n",
    "    np.savetxt(f\"Vals_{i}.out\", Vals_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ML workflow shown here depicts the process for 2 of the windows. The same process was followed for the other windows.\n",
    "- Following this, the wave peaks and troughs in the missing portion were reconstructed.\n",
    "- The imputed values for the 33 components were used to give the surface elevation value through a reconstruction process utilizing the same wave modeling equation.\n",
    "- Finally, the predicted and the true peak values were compared for the entire window and a mean absolute error was used to give an estimate of the imputation efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The comparisons of the CNN+LSTM model with other imputation approaches are shown in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
