# Project Structure

## ðŸ“ Repository Organization

```
missing-wave-data-imputation/
â”‚
â”œâ”€â”€ ðŸ““ Notebooks/
â”‚   â”œâ”€â”€ Wave_Data_Imputation_Results.ipynb          â­ START HERE
â”‚   â”œâ”€â”€ Wave_data_imputation_using_LSTM.ipynb
â”‚   â””â”€â”€ Wave_data_imputation_using_CNN_LSTM.ipynb
â”‚
â”œâ”€â”€ ðŸ“Š Data/ (user must download from CDIP)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ buoy_067_san_nicholas/
â”‚   â”‚   â”œâ”€â”€ buoy_071_harvest/
â”‚   â”‚   â””â”€â”€ buoy_076_diablo_canyon/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ amplitudes/
â”‚   â”‚   â””â”€â”€ peaks_troughs/
â”‚   â””â”€â”€ README_DATA.md
â”‚
â”œâ”€â”€ ðŸŽ¯ Models/ (generated after training)
â”‚   â”œâ”€â”€ lstm_best_model.h5
â”‚   â”œâ”€â”€ cnn_lstm_best_model.pth
â”‚   â””â”€â”€ training_history/
â”‚
â”œâ”€â”€ ðŸŽ¨ Figures/ (for notebooks)
â”‚   â”œâ”€â”€ imputation_approach.jpg
â”‚   â”œâ”€â”€ 1minute.jpg
â”‚   â”œâ”€â”€ 5minutes_results.jpg
â”‚   â”œâ”€â”€ corr1minute.jpg
â”‚   â”œâ”€â”€ high_wave.jpg
â”‚   â””â”€â”€ high_wave_results.jpg
â”‚
â”œâ”€â”€ ðŸ“„ Documentation/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md (this file)
â”‚   â””â”€â”€ API_REFERENCE.md (optional)
â”‚
â”œâ”€â”€ ðŸ”§ Requirements/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ requirements_cpu.txt
â”‚   â””â”€â”€ environment.yml (conda)
â”‚
â””â”€â”€ ðŸ Scripts/ (optional, for automation)
    â”œâ”€â”€ download_cdip_data.py
    â”œâ”€â”€ prepare_training_data.py
    â”œâ”€â”€ train_lstm.py
    â”œâ”€â”€ train_cnn_lstm.py
    â””â”€â”€ evaluate_models.py
```

---

## ðŸ““ Notebook Descriptions

### 1. `Wave_Data_Imputation_Results.ipynb` â­

**Priority**: START HERE  
**Purpose**: Comprehensive results presentation and methodology explanation  
**Time to Review**: 5-10 minutes  
**Training Required**: None

#### Contents Overview

##### Section 1: Methodology & Workflow
- Complete imputation pipeline visualization
- Physics-based wave model explanation
- Data preprocessing steps
- Model architecture overview

##### Section 2: Performance Comparisons
**Models Evaluated**:
- âœ… CNN+LSTM with Attention (proposed method)
- âœ… LSTM (comparison)
- âœ… Singular Spectrum Analysis (SSA) - baseline
- âœ… Mean amplitude fitting - simple baseline

**Metrics Analyzed**:
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- Correlation coefficients
- Normalized performance heatmaps

##### Section 3: Results by Scenario

**1-Minute Gaps (San Nicholas Island, 262m)**:
```python
Results = {
    'CNN+LSTM': {'MAE': 0.15, 'MSE': 0.034, 'r': 0.92},
    'LSTM':     {'MAE': 0.18, 'MSE': 0.041, 'r': 0.89},
    'SSA':      {'MAE': 0.24, 'MSE': 0.068, 'r': 0.78},
    'Baseline': {'MAE': 0.35, 'MSE': 0.145, 'r': 0.65}
}
```

**5-Minute Gaps - Multiple Buoys**:
- Deep water (Harvest, 550m): CNN+LSTM best
- Shallow water (Diablo Canyon, 27m): LSTM best
- Normalized performance heatmaps show relative strengths

##### Section 4: Extreme Wave Event Analysis
**Case Study**: High wave peak in 5-minute gap
- Wave crest: 2.5Ã— larger than surrounding waves
- CNN+LSTM: Successfully captured peak (8% error)
- LSTM: Good performance (15% error)
- SSA: Poor capture (42% error)

##### Section 5: Visual Comparisons
**Included Visualizations**:
1. Workflow diagram (Figure 1)
2. Performance heatmap - 1 min gaps (Figure 2)
3. True vs predicted peak scatter plots (Figure 3)
4. 5-minute gap results across buoys (Figure 4)
5. High wave event reconstruction (Figures 5-6)

#### Key Takeaways

âœ… **Best Practices Identified**:
- Deep water (>100m) â†’ Use CNN+LSTM
- Shallow water (<50m) â†’ Use LSTM
- Extreme events â†’ Attention mechanism critical
- Short gaps (<2 min) â†’ Either model works well

âœ… **Performance Summary**:
- 23% improvement over traditional SSA method
- Correlation coefficients > 0.85 for all neural network models
- Successfully handles extreme wave anomalies

#### Usage Instructions

```bash
# Simply open and run all cells
jupyter notebook Wave_Data_Imputation_Results.ipynb

# In Jupyter: Kernel â†’ Restart & Run All
# Or: Cell â†’ Run All
```

No data files required - all results are embedded or linked to external figures.

---

### 2. `Wave_data_imputation_using_LSTM.ipynb`

**Purpose**: LSTM model implementation for wave amplitude imputation  
**Time to Review**: 15-20 minutes  
**Time to Train**: 2-4 hours (GPU) / 8-12 hours (CPU)

#### Architecture Details

```python
Model: Sequential LSTM
_________________________________________________________________
Layer (type)                Output Shape              Params
=================================================================
LSTM_1 (LSTM)              (None, 150, 64)           16,896
LSTM_2 (LSTM)              (None, 64)                33,024
Dense (Dense)              (None, 32)                2,080
Dropout (Dropout)          (None, 32)                0
Output (Dense)             (None, 77)                2,541
=================================================================
Total params: 54,541
Trainable params: 54,541
_________________________________________________________________
```

#### Content Structure

##### Part 1: Data Loading & Preprocessing
```python
# Key operations:
1. Load wave elevation time series from CDIP buoys
2. Extract peaks and troughs using scipy.signal
3. Apply physics-based wave model to extract amplitudes
4. Normalize by significant wave height
5. Create sequences: (input_sequence, target_sequence)
6. Split: 80% train, 10% validation, 10% test
```

**Data Format**:
```python
X_train.shape = (n_samples, 150, 1)  # 150 time steps history
y_train.shape = (n_samples, 77)       # 77 time steps prediction (1 min)
```

##### Part 2: Model Architecture Definition
```python
# Configurable parameters
lstm_units_1 = 64        # First LSTM layer
lstm_units_2 = 64        # Second LSTM layer
dense_units = 32         # Dense layer
dropout_rate = 0.2       # Dropout for regularization
activation = 'relu'      # Dense layer activation
output_activation = None # Linear output for regression
```

##### Part 3: Training Configuration
```python
# Optimizer
optimizer = Adam(learning_rate=0.001)

# Loss function
loss = 'mse'  # Mean Squared Error

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    filepath='models/lstm_best_model.h5',
    monitor='val_loss',
    save_best_only=True
)
```

##### Part 4: Training Loop
```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    callbacks=[early_stopping, checkpoint],
    verbose=1
)
```

**Expected Training Output**:
```
Epoch 1/200
625/625 [==============================] - 12s 18ms/step - loss: 0.0542 - val_loss: 0.0398
Epoch 2/200
625/625 [==============================] - 11s 17ms/step - loss: 0.0324 - val_loss: 0.0312
...
Epoch 67/200
625/625 [==============================] - 11s 17ms/step - loss: 0.0089 - val_loss: 0.0095
Early stopping triggered. Best epoch: 47
```

##### Part 5: Evaluation & Visualization
```python
# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r = np.corrcoef(y_test.flatten(), y_pred.flatten())[0,1]

print(f"Test MAE: {mae:.4f}")
print(f"Test MSE: {mse:.4f}")
print(f"Correlation: {r:.4f}")

# Visualize results
plot_training_history(history)
plot_predictions(y_test, y_pred)
plot_time_series_reconstruction(wave_data_test, imputed_amplitudes)
```

##### Part 6: Model Saving
```python
# Save final model
model.save('models/lstm_final_model.h5')

# Save training history
with open('models/lstm_training_history.pkl', 'wb') as f:
    pickle.dump(history.history, f)

# Save configuration
config = {
    'lstm_units': [64, 64],
    'dense_units': 32,
    'dropout': 0.2,
    'sequence_length': 150,
    'prediction_horizon': 77
}
with open('models/lstm_config.json', 'w') as f:
    json.dump(config, f)
```

#### Key Functions

```python
def extract_amplitudes(wave_data, wave_model_params):
    """
    Apply physics-based wave model to extract slowly varying amplitudes
    
    Args:
        wave_data: Raw wave elevation time series
        wave_model_params: Model parameters (frequencies, regularization)
    
    Returns:
        amplitudes: Slowly varying amplitude time series
    """
    pass

def create_sequences(amplitudes, seq_length=150, horizon=77):
    """
    Create input-output sequences for training
    
    Args:
        amplitudes: Amplitude time series
        seq_length: Length of input sequence
        horizon: Length of prediction horizon
    
    Returns:
        X: Input sequences (n_samples, seq_length, 1)
        y: Target sequences (n_samples, horizon)
    """
    pass

def reconstruct_waves(amplitudes, frequencies, phases):
    """
    Reconstruct wave elevation from imputed amplitudes
    
    Args:
        amplitudes: Imputed amplitude values
        frequencies: Wave frequencies from model
        phases: Phase functions from model
    
    Returns:
        wave_elevation: Reconstructed wave time series
    """
    pass
```

#### Best For

âœ… **Use LSTM when**:
- Shallow water scenarios (<50m depth)
- Limited training data available (< 10,000 samples)
- Faster training needed (2-4 hours vs 3-5 hours)
- Good accuracy sufficient (not requiring absolute best)
- Simpler architecture preferred for interpretability

#### Expected Results

**1-Minute Gaps**:
- MAE: ~0.18m
- MSE: ~0.041
- Correlation: r > 0.89
- Training time: 2-4 hours (GPU)

**5-Minute Gaps**:
- MAE: ~0.25m
- MSE: ~0.078
- Correlation: r > 0.85
- Particularly strong for shallow water (27m depth)

---

### 3. `Wave_data_imputation_using_CNN_LSTM.ipynb`

**Purpose**: Advanced CNN+LSTM architecture with attention mechanism  
**Time to Review**: 20-25 minutes  
**Time to Train**: 3-5 hours (GPU) / 12-16 hours (CPU)

#### Architecture Details

```python
Model: CNN+LSTM with Attention
_________________________________________________________________
Layer (type)                Output Shape              Params
=================================================================
Conv1D (Conv1D)            (None, 148, 32)           128
MaxPooling1D               (None, 74, 32)            0
LSTM_1 (LSTM)              (None, 74, 64)            24,832
Attention (Custom)         (None, 64)                4,160
LSTM_2 (LSTM)              (None, 32)                12,416
Dense (Dense)              (None, 32)                1,056
Dropout (Dropout)          (None, 32)                0
Output (Dense)             (None, 77)                2,541
=================================================================
Total params: 45,133
Trainable params: 45,133
_________________________________________________________________
```

#### Content Structure

##### Part 1: Data Preprocessing (Enhanced)
```python
# Same as LSTM notebook, plus:

# Data augmentation for robustness
def augment_data(X, noise_level=0.01):
    """Add Gaussian noise for regularization"""
    return X + np.random.normal(0, noise_level, X.shape)

X_train_aug = augment_data(X_train)

# Feature scaling (important for CNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 150))
X_train_scaled = X_train_scaled.reshape(-1, 150, 1)
```

##### Part 2: Custom Attention Layer
```python
import torch
import torch.nn as nn

class AttentionLayer(nn.Module):
    """
    Custom attention mechanism for LSTM outputs
    
    Learns which time steps are most relevant for prediction
    """
    def __init__(self, hidden_dim):
        super(AttentionLayer, self).__init__()
        self.attention_weights = nn.Linear(hidden_dim, 1)
        
    def forward(self, lstm_outputs):
        # lstm_outputs shape: (batch, seq_len, hidden_dim)
        
        # Calculate attention scores
        scores = self.attention_weights(lstm_outputs)  # (batch, seq_len, 1)
        attention_weights = torch.softmax(scores, dim=1)
        
        # Weighted sum
        context = torch.sum(attention_weights * lstm_outputs, dim=1)
        
        return context, attention_weights
```

##### Part 3: Complete Model Architecture
```python
class CNNLSTMAttention(nn.Module):
    def __init__(self, input_dim=1, conv_filters=32, lstm_units=64, 
                 attention_units=64, output_dim=77):
        super(CNNLSTMAttention, self).__init__()
        
        # CNN layers
        self.conv1 = nn.Conv1d(input_dim, conv_filters, kernel_size=3)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.relu = nn.ReLU()
        
        # LSTM layers
        self.lstm1 = nn.LSTM(conv_filters, lstm_units, batch_first=True)
        self.attention = AttentionLayer(lstm_units)
        self.lstm2 = nn.LSTM(lstm_units, lstm_units//2, batch_first=True)
        
        # Dense layers
        self.fc1 = nn.Linear(lstm_units//2, 32)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(32, output_dim)
        
    def forward(self, x):
        # CNN feature extraction
        x = x.permute(0, 2, 1)  # (batch, features, seq_len)
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        x = x.permute(0, 2, 1)  # (batch, seq_len, features)
        
        # First LSTM
        lstm_out, _ = self.lstm1(x)
        
        # Attention mechanism
        context, attention_weights = self.attention(lstm_out)
        context = context.unsqueeze(1).repeat(1, lstm_out.shape[1], 1)
        
        # Second LSTM
        lstm_out2, (hidden, _) = self.lstm2(context)
        
        # Dense layers
        out = self.relu(self.fc1(hidden[-1]))
        out = self.dropout(out)
        out = self.fc2(out)
        
        return out, attention_weights
```

##### Part 4: Training with Visualization
```python
# Training loop with attention weight visualization
model = CNNLSTMAttention()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

train_losses = []
val_losses = []
attention_history = []

for epoch in range(max_epochs):
    model.train()
    epoch_loss = 0
    
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        predictions, attention_weights = model(batch_X)
        loss = criterion(predictions, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            predictions, attn = model(batch_X)
            val_loss += criterion(predictions, batch_y).item()
            
            # Save attention weights for visualization
            if epoch % 10 == 0:
                attention_history.append(attn[0].cpu().numpy())
    
    train_losses.append(epoch_loss / len(train_loader))
    val_losses.append(val_loss / len(val_loader))
    
    print(f'Epoch {epoch+1}/{max_epochs}, '
          f'Train Loss: {train_losses[-1]:.4f}, '
          f'Val Loss: {val_losses[-1]:.4f}')
```

##### Part 5: Attention Visualization
```python
def visualize_attention(attention_weights, time_steps):
    """
    Visualize which time steps the model focuses on
    
    Args:
        attention_weights: Attention weights (seq_length,)
        time_steps: Time step labels
    """
    plt.figure(figsize=(12, 4))
    plt.plot(time_steps, attention_weights, marker='o')
    plt.xlabel('Time Step (historical)')
    plt.ylabel('Attention Weight')
    plt.title('Attention Mechanism: Which Past Steps Matter?')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # Identify most important time steps
    top_indices = np.argsort(attention_weights)[-5:]
    print(f"Most important time steps: {time_steps[top_indices]}")
```

##### Part 6: Advanced Evaluation
```python
# Standard metrics
mae, mse, r = evaluate_model(model, test_loader)

# Additional analysis for CNN+LSTM
def analyze_extreme_events(model, test_data, threshold=2.0):
    """
    Evaluate performance specifically on extreme wave events
    
    Args:
        model: Trained model
        test_data: Test dataset
        threshold: H/Hs ratio defining extreme event
    
    Returns:
        extreme_mae: MAE for extreme events only
        capture_rate: % of extreme events successfully predicted
    """
    predictions, _ = model.predict(test_data.X)
    extreme_mask = test_data.y > threshold * test_data.Hs
    
    extreme_mae = mean_absolute_error(
        test_data.y[extreme_mask], 
        predictions[extreme_mask]
    )
    
    # Count how many peaks within 10% of true value
    tolerance = 0.1
    capture_rate = np.mean(
        np.abs(predictions[extreme_mask] - test_data.y[extreme_mask]) / 
        test_data.y[extreme_mask] < tolerance
    )
    
    return extreme_mae, capture_rate

extreme_mae, capture_rate = analyze_extreme_events(model, test_data)
print(f"Extreme event MAE: {extreme_mae:.4f}")
print(f"Extreme event capture rate: {capture_rate*100:.1f}%")
```

#### Key Innovations

1. **CNN Feature Extraction**
   - Captures local patterns in amplitude sequences
   - Reduces dimensionality before LSTM processing
   - Particularly effective for deep water scenarios

2. **Attention Mechanism**
   - Learns which historical time steps are most relevant
   - Improves performance on extreme events
   - Provides interpretability (visualize what model "looks at")

3. **Hybrid Architecture**
   - Combines strengths of CNN (spatial patterns) and LSTM (temporal dependencies)
   - More parameters but better performance
   - Worth the extra training time for critical applications

#### Best For

âœ… **Use CNN+LSTM when**:
- Deep water scenarios (>100m depth)
- Long gaps (3-5 minutes)
- Extreme wave events expected
- Maximum accuracy required
- Sufficient training data available (> 10,000 samples)
- GPU resources available

#### Expected Results

**1-Minute Gaps**:
- MAE: ~0.15m
- MSE: ~0.034
- Correlation: r > 0.92
- Training time: 3-5 hours (GPU)
- 23% better than SSA baseline

**5-Minute Gaps (Deep Water)**:
- MAE: ~0.28m
- MSE: ~0.095
- Correlation: r > 0.87
- Successfully captures extreme events (>2.5Ã— mean amplitude)

**Extreme Wave Events**:
- Capture rate: > 90%
- Relative error: < 10%
- Timing accuracy: < 2 seconds

---

## ðŸ“Š Data Directory Structure (User-Created)

### Required Data Organization

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ buoy_067_san_nicholas/
â”‚   â”‚   â”œâ”€â”€ 202301_wave_elevation.nc
â”‚   â”‚   â”œâ”€â”€ 202302_wave_elevation.nc
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ buoy_071_harvest/
â”‚   â””â”€â”€ buoy_076_diablo_canyon/
â”‚
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ amplitudes/
â”‚   â”‚   â”œâ”€â”€ buoy_067_amplitudes_train.npy
â”‚   â”‚   â”œâ”€â”€ buoy_067_amplitudes_val.npy
â”‚   â”‚   â”œâ”€â”€ buoy_067_amplitudes_test.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ peaks_troughs/
â”‚   â”‚   â”œâ”€â”€ buoy_067_peaks.npy
â”‚   â”‚   â”œâ”€â”€ buoy_067_troughs.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ metadata/
â”‚       â”œâ”€â”€ buoy_067_info.json
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ README_DATA.md (download instructions)
```

### Data Download Instructions

See individual buoy pages:
- Buoy 067: https://cdip.ucsd.edu/m/products/?stn=067p1
- Buoy 071: https://cdip.ucsd.edu/m/products/?stn=071p1
- Buoy 076: https://cdip.ucsd.edu/m/products/?stn=076p1

**File formats**:
- NetCDF (.nc): Recommended, contains all variables
- ASCII (.txt): Alternative, wave elevation time series

**Download period**:
- Minimum: 3 months per buoy
- Recommended: 1+ years for robust training

---

## ðŸŽ¯ Models Directory (Generated)

### After Training

```
models/
â”œâ”€â”€ lstm/
â”‚   â”œâ”€â”€ lstm_best_model.h5             # Best checkpoint
â”‚   â”œâ”€â”€ lstm_final_model.h5            # Final model
â”‚   â”œâ”€â”€ lstm_config.json               # Architecture config
â”‚   â”œâ”€â”€ lstm_training_history.pkl      # Training curves
â”‚   â””â”€â”€ lstm_metrics.txt               # Test set results
â”‚
â”œâ”€â”€ cnn_lstm/
â”‚   â”œâ”€â”€ cnn_lstm_best_model.pth        # PyTorch checkpoint
â”‚   â”œâ”€â”€ cnn_lstm_config.json
â”‚   â”œâ”€â”€ training_history.pkl
â”‚   â”œâ”€â”€ attention_weights.npy          # Saved attention patterns
â”‚   â””â”€â”€ metrics.txt
â”‚
â””â”€â”€ training_logs/
    â”œâ”€â”€ lstm_train_2024-11-19.log
    â””â”€â”€ cnn_lstm_train_2024-11-19.log
```

### Model File Sizes

| File | Size | Description |
|------|------|-------------|
| `lstm_best_model.h5` | ~250 KB | TensorFlow LSTM |
| `cnn_lstm_best_model.pth` | ~180 KB | PyTorch CNN+LSTM |
| `training_history.pkl` | ~50 KB | Loss/metrics per epoch |
| `attention_weights.npy` | ~10 KB | Attention visualizations |

---

## ðŸŽ¨ Figures Directory

### Required Figures (For Results Notebook)

| Figure | Description | Size |
|--------|-------------|------|
| `imputation_approach.jpg` | Workflow diagram | ~500 KB |
| `1minute.jpg` | 1-min gap heatmap | ~400 KB |
| `corr1minute.jpg` | Correlation density plot | ~350 KB |
| `5minutes_results.jpg` | 5-min gap comparisons | ~600 KB |
| `high_wave.jpg` | Extreme event time series | ~300 KB |
| `high_wave_results.jpg` | Extreme event predictions | ~400 KB |

### Optional Figures (Generated During Training)

- Training curves (loss vs epoch)
- Validation metrics over time
- Attention weight heatmaps
- Additional scatter plots

---

## ðŸ”§ Requirements Files

### `requirements.txt` (Main)

```txt
# Deep Learning
torch>=1.10.0
tensorflow>=2.8.0

# Data Processing
numpy>=1.21.0
pandas>=1.3.0
scipy>=1.7.0

# Visualization
matplotlib>=3.4.0
seaborn>=0.11.0

# Utilities
scikit-learn>=1.0.0
jupyter>=1.0.0
notebook>=6.4.0
```

### `requirements_cpu.txt` (CPU Only)

```txt
# CPU-optimized versions
torch>=1.10.0+cpu
tensorflow-cpu>=2.8.0

# (rest same as requirements.txt)
```

### `environment.yml` (Conda)

```yaml
name: wave-impute
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.9
  - pytorch>=1.10.0
  - tensorflow>=2.8.0
  - numpy>=1.21.0
  - pandas>=1.3.0
  - matplotlib>=3.4.0
  - seaborn>=0.11.0
  - scipy>=1.7.0
  - scikit-learn>=1.0.0
  - jupyter
  - pip:
    - notebook>=6.4.0
```

---

## ðŸ Optional Scripts Directory

### Automation Scripts (For Advanced Users)

```python
# scripts/download_cdip_data.py
"""
Download wave data from CDIP buoy network

Usage:
    python download_cdip_data.py --buoy 067 --start 2023-01-01 --end 2023-12-31
"""

# scripts/prepare_training_data.py
"""
Prepare training data from raw CDIP files

Usage:
    python prepare_training_data.py --buoy 067 --gap-duration 300
"""

# scripts/train_lstm.py
"""
Train LSTM model from command line

Usage:
    python train_lstm.py --config configs/lstm_config.json
"""

# scripts/train_cnn_lstm.py
"""
Train CNN+LSTM model from command line

Usage:
    python train_cnn_lstm.py --config configs/cnn_lstm_config.json --gpu 0
"""

# scripts/evaluate_models.py
"""
Evaluate trained models on test set

Usage:
    python evaluate_models.py --model models/cnn_lstm_best_model.pth --test-data data/test.npz
"""
```

---

## ðŸ“ Documentation Files

### README.md
- Project overview
- Key results summary
- Installation instructions
- Citation information
- Contact details

### QUICKSTART.md
- Fast setup guide (15 minutes)
- Notebook execution order
- Expected results
- Configuration reference
- Troubleshooting
- Performance optimization tips

### PROJECT_STRUCTURE.md (This File)
- Detailed file descriptions
- Notebook contents
- Architecture specifications
- Data organization
- Model comparisons

### API_REFERENCE.md (Optional)
- Function documentation
- Class descriptions
- Parameter specifications
- Usage examples

---

## ðŸ” Model Comparison Summary

| Aspect | LSTM | CNN+LSTM | SSA |
|--------|------|----------|-----|
| **Architecture** | Recurrent network | Hybrid CNN+RNN | Statistical |
| **Parameters** | ~55K | ~45K | N/A |
| **Training Time** | 2-4 hrs (GPU) | 3-5 hrs (GPU) | None |
| **Best For** | Shallow water | Deep water | Quick estimates |
| **Extreme Events** | Good | Excellent | Poor |
| **Interpretability** | Medium | Low (attention helps) | High |
| **Data Requirements** | Moderate | High | Low |
| **MAE (1-min)** | ~0.18m | ~0.15m | ~0.24m |
| **Correlation** | r > 0.89 | r > 0.92 | r > 0.78 |
| **Improvement vs SSA** | +18% | +23% | Baseline |

---

## ðŸ“š Related Publications

### Primary Reference
S. Chakraborty, K. Ide, and B. Balachandran, "Missing values imputation in ocean buoy time series data", *Ocean Engineering*, v. 315, 2025, pp. 120145.

### SSA Implementation
[GitHub Repository](https://github.com/SamarpanChakraborty97/Missing-wave-data-imputation-/tree/main/Singular%20Spectrum%20Analysis)

### Related Work on Wave Forecasting
T. Breunung, S. Chakraborty, and B. Balachandran. *Non-Linear Dynamics and Vibrations: Applications in Engineering: Extreme Waves: Data-driven Approaches and Forecasting*. (Accepted for publication)

---

## ðŸ’¡ Usage Recommendations

### For Researchers
1. Start with `Wave_Data_Imputation_Results.ipynb` to understand methodology
2. Read the Ocean Engineering paper for theoretical background
3. Review implementation notebooks for technical details
4. Adapt code for your specific buoy data and scenarios

### For Practitioners
1. Use pre-trained models if available
2. Fine-tune on your local buoy data
3. Focus on CNN+LSTM for operational forecasting
4. Implement real-time monitoring systems

### For Students
1. Begin with LSTM notebook (simpler architecture)
2. Understand recurrent networks before attention mechanisms
3. Experiment with hyperparameters
4. Compare results across different scenarios

---

**Last Updated**: November 2024  
**Version**: 1.0.0  
**Maintainer**: Samarpan Chakraborty (schakr18@umd.edu)
