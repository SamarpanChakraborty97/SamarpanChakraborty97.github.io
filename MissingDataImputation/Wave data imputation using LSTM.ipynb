{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended for imputation of missing values in time series datasets obtained from wave buoys using Long-Short Term Memory (LSTM) networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RogueWaves](https://www.quantamagazine.org/wp-content/uploads/2020/02/RogueWave_2880x1620-Lede.jpg)\n",
    "[Source](https://www.quantamagazine.org/the-grand-unified-theory-of-rogue-waves-20200205/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The data is obtained from the [CDIP buoys](https://cdip.ucsd.edu/m/stn_table/) website, where wave surface elevation data from different surface buoys in the form of netcdf files are obtained by using a custom $\\text{MATLAB}$ script. The netcdf files contain historic as well as real-time data of different measured wave variables. \n",
    "\n",
    "- **The objective was to predict possible sudden high-amplitude waves (wave annomalies (better known as *rogue waves*) that might be present in the missing piece of data. These waves occur suddenly in the time scale of seconds, hence it made sense to utilize the records of an observed variable which has a high sampling rate.**\n",
    "- Keeping this in mind, the wave surface elevation data which has a sampling rate of 1.28*Hz* was used. Other buoy variables, either did not directly capture the wave magnitudes or had a much lower sampling rate (order of minutes).\n",
    "- Thus, it made sense to use surface wave elevation data for the wave data imputation exercise.\n",
    "- The data is acquired in the form of windows spanning from 20 to 25 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data pre-processing\n",
    "- The wave peaks and troughs are filtered, since they capture the extreme components in the time series.\n",
    "- These are then broken down to two sections with a specific duration of data missing between them. The objective is to impute *these peakls in the time series data*.   \n",
    "- The $\\text{MATLAB}$ script above employs a wave modelling equation to fit the obtained data peak values in the time windows. This involves the use of a *regularization parameter $\\alpha$* which penalizes the extreme values of wave magnitudes.\n",
    "- Along with this, number of Fourier components used for breaking down the wave data were also investigated to give the best fit.\n",
    "- The fit data was then broken down into individual time series datasets corresponding to the number of Fourier components. These qualify as the final processed datasets to be used for the ML and other quantitative modelling techniques.\n",
    "- To summarize, we had ***100 data windows*** and ***N = 33 components***. This resulted in **33 datasets corresponding to the portion preceding the missing data, 33 datasets corresponding to the portion following the missing data for each window**.\n",
    "- Finally, a lagged table was created for each of these components and **33 total datasets for 1 window** were created by combining the trailing and leading portions. A lag value of ***M=200*** was chosen after parametric studies. This corresponded to the case which resulted in the **lowest validation error** for a sample window.\n",
    "- Tools used: ***pandas***, ***Numpy***      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wave_fit_data](Figure4.jpg)\n",
    "**A comparison of the effect of the *regularization parameter $\\alpha$* on the fit of the wave data in a particular window. Larger the parameter, more conservative is the fit (more the regularization effect)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model Workflow \n",
    "- The model workflow comprises of the following processes:\n",
    "    1. Reading the pre-processed data using **pandas** and **Numpy**.\n",
    "    2. Creating the lagged table for each of the components.\n",
    "    3. Standardizing the features and creating the train, validation and test datasets.\n",
    "    4. Training the LSTM model using custom optimizer class on the data using **PyTorch**.\n",
    "    5. Validating the trained LSTM models througha early stopping mechanism\n",
    "    6. Storing the trained model.\n",
    "    7. Evaluating the test data on the trained model.\n",
    "- The workflow thus discussed constitutes **the imputed values for 1 of the 33 components**. A for-loop is used for imputation of missing values for all the 33 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv1d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool1d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import datetime as datetime\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim,  output_dim, dropout_prob, hidden_dim, layer_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.d_prob = dropout_prob\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(self.d_prob)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagged table generation\n",
    "A function to create the training and validation datasets by creating lag values for a time series forecasting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLaggedDf(df, training_len, missing_len, col_name):\n",
    "    df_new = df.copy()\n",
    "    for i in range(1,training_len + missing_len):\n",
    "        df_new[f\"Lag{i}\"] = df[[col_name]].shift(i)\n",
    "    df_new = df_new.iloc[training_len + missing_len:]\n",
    "    \n",
    "    df_new = df_new.dropna(axis= 0)\n",
    "    \n",
    "    mid_df = int(0.5 * len(df_new))\n",
    "    \n",
    "    df_1 = df_new[:mid_df]\n",
    "    df_2 = df_new[mid_df:]\n",
    "   \n",
    "    train_len1 = int(0.8 * len(df_1))\n",
    "    train_len2 = int(0.8 * len(df_2))\n",
    "    \n",
    "    df_train1 = df_1[:train_len1]\n",
    "    df_val1 = df_1[train_len1:]\n",
    "    \n",
    "    df_train2 = df_2[:train_len2]\n",
    "    df_val2 = df_2[train_len2:]\n",
    "    \n",
    "    df_train = df_train1.append(df_train2, ignore_index = True)\n",
    "    df_val = df_val1.append(df_val2, ignore_index = True)\n",
    "    \n",
    "    trainY = df_train.iloc[:,:missing_len]\n",
    "    trainX = df_train.drop(df_train.iloc[:,:missing_len], axis=1)\n",
    "    \n",
    "    valY = df_val.iloc[:,:missing_len]\n",
    "    valX = df_val.drop(df_train.iloc[:,:missing_len], axis=1)\n",
    "    \n",
    "    return trainX, trainY, valX, valY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom optimizer class to include the training, validation and loss plotting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer, patience, min_delta = 1e-6):\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.counter = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.patience = patience\n",
    "        \n",
    "    def train_step(self, x, y):\n",
    "\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def earlyStop(self, validation_loss):\n",
    "        if validation_loss < (self.min_validation_loss - self.min_delta):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "        elif validation_loss >= (self.min_validation_loss - self.min_delta):\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size, n_epochs, mode, n_features, output_dim):\n",
    "\n",
    "        model_path = f'lstm_1.pt'\n",
    "        break_out_flag = False\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "                if self.earlyStop(validation_loss):\n",
    "                    break_out_flag = True\n",
    "                    break               \n",
    "            \n",
    "            if break_out_flag:\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "                break\n",
    "\n",
    "            #if (epoch % 50 == 0):\n",
    "            #    print(\n",
    "            #        f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "            #    )\n",
    "        #torch.save(self.model.state_dict(), model_path)\n",
    "          \n",
    "    def evaluate(self, x, test):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for i in range(len(test)):\n",
    "                x = x.to(device)\n",
    "                self.model.eval()\n",
    "                x_test = x.view([1, -1, 100]).to(device)\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(test[i].to(device).detach().numpy())\n",
    "                #print(x.size())\n",
    "                #print(yhat.size())\n",
    "                x=torch.reshape(x,(100,1))\n",
    "                x = torch.cat((x,yhat),0)\n",
    "                x = x[1:]\n",
    "\n",
    "        return predictions, values\n",
    "    \n",
    "    def evaluate2(self, x, test, training_len, missing_len):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for j in range(len(test)):\n",
    "                val = test[j].to(device).cpu()\n",
    "                values.append(val.detach().numpy())\n",
    "            \n",
    "            num = len(test) % missing_len\n",
    "            if (num == 0):\n",
    "                for i in range(math.floor(len(test)/missing_len)):\n",
    "                    x = x.to(device)\n",
    "                    self.model.eval()\n",
    "                    x_test = x.view([1, -1, training_len]).to(device)\n",
    "                \n",
    "                    yhat = self.model(x_test)\n",
    "                    yint = torch.reshape(yhat,(missing_len,1))                \n",
    "                    y_int = yint.to(device).cpu()\n",
    "                    predictions.append(y_int.detach().numpy())\n",
    "                    x = torch.reshape(x,(training_len,1))\n",
    "                    x = torch.cat((x,yint),0)\n",
    "                    x = x[-training_len:]\n",
    "            else:\n",
    "                for i in range(math.floor(len(test)/missing_len)+1):\n",
    "                    x = x.to(device)\n",
    "                    self.model.eval()\n",
    "                    x_test = x.view([1, -1, training_len]).to(device)\n",
    "                \n",
    "                    yhat = self.model(x_test)\n",
    "                    yint = torch.reshape(yhat,(missing_len,1))                \n",
    "                    y_int = yint.to(device).cpu()\n",
    "                    predictions.append(y_int.detach().numpy())\n",
    "                    x = torch.reshape(x,(training_len,1))\n",
    "                    x = torch.cat((x,yint),0)\n",
    "                    x = x[-training_len:]\n",
    "            \n",
    "        preds =  torch.reshape(torch.Tensor(predictions),(-1,1))\n",
    "        \n",
    "        return np.asarray(values), np.asarray(preds)\n",
    "\n",
    "\n",
    "    def plot_losses(self, training_len):\n",
    "        \"\"\"The method plots the calculated loss values for training and validation\n",
    "        \"\"\"\n",
    "        np.savetxt(f\"Output_length={training_len}_train.out\", self.train_losses, fmt='%1.4e')\n",
    "        np.savetxt(f\"Output_length={training_len}_val.out\", self.val_losses, fmt='%1.4e')\n",
    "        \n",
    "        plt.figure(figsize=[10,8])\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Losses for output length = {training_len}\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        plt.savefig(f'Losses comparisons for output length={training_len} over epochs.png',dpi=300)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initially, different RNN models were investigated. The LSTM model was chosen as it resulted in the lowest validation errors over different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        #\"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel,\n",
    "        #\"bi-lstm\": BiLSTMModel,\n",
    "        #\"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on the main data\n",
    "- The data is read from the .csv files here. \n",
    "- The diffent functions are called here. \n",
    "- The data is broken down into training, validation datasets after scaling them. \n",
    "- The missing data is taken as the test dataset. \n",
    "- The hyperparameters shown here are arrived at after extensive hyperparameter tuning *(input dimension, hidden dimesnion, layer dimension, dropout probability, laerning rate, batch size)*\n",
    "- Finally, the missing portion was imputed through iterative single step ahead predictions and the predictions and values stored for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for i in range(1,21):\n",
    "    data_pre = pd.read_csv(f\"Slow_amp_pre_{i}.csv\", header=None)\n",
    "    data_post = pd.read_csv(f\"Slow_amp_post_{i}.csv\", header=None)\n",
    "    data_whole = pd.read_csv(f\"Slow_amp_whole_{i}.csv\", header=None)\n",
    "    \n",
    "    n_rows = data_pre.shape[0]\n",
    "    n_cols = data_whole.shape[1] - (data_pre.shape[1] + data_post.shape[1])\n",
    "\n",
    "    data_miss = pd.DataFrame(np.zeros([n_rows, n_cols])*np.nan)\n",
    "    \n",
    "    data_pre_vals = data_pre[:].values\n",
    "    data_post_vals = data_post[:].values\n",
    "    data_whole_vals = data_whole[:].values\n",
    "    \n",
    "    data_test = scaler.fit_transform(data_whole_vals.reshape(-1,1)).reshape(data_whole_vals.shape[0],data_whole_vals.shape[1])\n",
    "    missing_len = data_miss.shape[1]\n",
    "    \n",
    "    Predictions_pre = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "    Values = np.zeros([data_miss.shape[0], data_miss.shape[1]])\n",
    "\n",
    "    dummy_data = pd.concat([data_pre, data_post], axis=1,ignore_index=True)\n",
    "\n",
    "    data = scaler.fit_transform(dummy_data[:].values.reshape(-1,1)).reshape(dummy_data.shape[0],dummy_data.shape[1])\n",
    "\n",
    "    pre_data_scaled = data[:,:data_pre.shape[1]]\n",
    "\n",
    "    pre_shape = pre_data_scaled.shape\n",
    "    \n",
    "    input_len = 200\n",
    "    output_len = 1\n",
    "\n",
    "    input_dim = input_len\n",
    "    output_dim = output_len\n",
    "    hidden_dim = 200\n",
    "    layer_dim = 2\n",
    "    dropout = 0.3\n",
    "    weight_decay = 1e-3\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    L1 = data.shape[1] - input_len\n",
    "\n",
    "    for j in range(len(data)):\n",
    "    #for j in range(2):\n",
    "        train_len1 = int(0.7 * L1)\n",
    "\n",
    "        X_pre = np.zeros([L1, input_len])\n",
    "        Y_pre = np.zeros([L1, output_len])\n",
    "\n",
    "        for k in range(L1):\n",
    "            X_pre[k,:] = data[j,k:k+input_len]\n",
    "            Y_pre[k,:] = data[j,k+input_len:k+input_len+output_len]\n",
    "\n",
    "        Train_X_pre = X_pre[:train_len1]\n",
    "        Train_Y_pre = Y_pre[:train_len1]\n",
    "        \n",
    "        Val_X_pre = X_pre[train_len1:]        \n",
    "        Val_Y_pre = Y_pre[train_len1:]\n",
    "    \n",
    "        X_train_pre_tensor = torch.Tensor(Train_X_pre.copy())\n",
    "        Y_train_pre_tensor = torch.Tensor(Train_Y_pre.copy())\n",
    "    \n",
    "        X_val_pre_tensor = torch.Tensor(Val_X_pre.copy())\n",
    "        Y_val_pre_tensor = torch.Tensor(Val_Y_pre.copy())\n",
    "    \n",
    "        mode = j\n",
    "    \n",
    "        torch.manual_seed(2)\n",
    "    \n",
    "        train_pre_eta = TensorDataset(X_train_pre_tensor, Y_train_pre_tensor)\n",
    "        val_pre_eta = TensorDataset(X_val_pre_tensor, Y_val_pre_tensor)\n",
    "    \n",
    "        start = timer()\n",
    "    \n",
    "        model_params = {'input_dim': input_dim,\n",
    "                    'hidden_dim' : hidden_dim,\n",
    "                    'layer_dim' : layer_dim,\n",
    "                    'output_dim' : output_dim,\n",
    "                    'dropout_prob' : dropout}\n",
    "\n",
    "        model = get_model('lstm', model_params)\n",
    "        model = model.to(device)\n",
    "\n",
    "        batch_size = 32\n",
    "        n_epochs = 1500\n",
    "\n",
    "        learning_rate = 1e-5\n",
    "        loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        #loss_fn = MeanCubeLoss()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "        Ftrain_loader = DataLoader(train_pre_eta, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        Fval_loader = DataLoader(val_pre_eta, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "        opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer, patience = 30)\n",
    "        opt.train(Ftrain_loader, Fval_loader, batch_size=batch_size, n_epochs=n_epochs, mode=i, n_features=input_dim, output_dim = output_dim)\n",
    "        opt.plot_losses(output_dim)\n",
    "            \n",
    "        end = timer()\n",
    "\n",
    "        dur = (end-start)/60\n",
    "        print(f'The total duration for the training is {dur} minutes')\n",
    "\n",
    "        X_Test = np.asarray(data_test[j,data_pre_vals.shape[1]-input_len:data_pre_vals.shape[1]])\n",
    "        Y_Test = np.asarray(data_test[j,data_pre_vals.shape[1]:data_pre_vals.shape[1]+missing_len])\n",
    "\n",
    "        Test_features = torch.Tensor(X_Test)\n",
    "        Test_targets = torch.Tensor(Y_Test)\n",
    "\n",
    "        #model = get_model('lstm', model_params)\n",
    "        #model = model.to(device)\n",
    "\n",
    "        PATH = f'lstm_1.pt'\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "        bl1 = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer, patience = 50)\n",
    "        values, preds = bl1.evaluate2(Test_features,Test_targets, input_len, output_len)\n",
    "    \n",
    "        num = len(Test_targets) % output_len\n",
    "\n",
    "        if (num != 0):\n",
    "            preds = preds[:len(Test_targets)]\n",
    " \n",
    "        p = np.asarray(preds).reshape(missing_len)\n",
    "        Predictions_pre[j,:] = p\n",
    "        Values[j,:] = values\n",
    "\n",
    "    Preds_rescaled = scaler.inverse_transform(Predictions_pre.reshape(-1,1)).reshape(Predictions_pre.shape[0],Predictions_pre.shape[1])\n",
    "    Vals_rescaled = scaler.inverse_transform(Values.reshape(-1,1)).reshape(Predictions_pre.shape[0],Predictions_pre.shape[1])\n",
    "    \n",
    "    np.savetxt(f\"Preds_lstm_{i}.out\", Preds_rescaled)\n",
    "    np.savetxt(f\"Vals_{i}.out\", Vals_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ML workflow shown here depicts the process for 20 of the windows. The same process was followed for the other windows.\n",
    "- Following this, the wave peaks and troughs in the missing portion were reconstructed.\n",
    "- The imputed values for the 33 components were used to give the surface elevation value through a reconstruction process utilizing the same wave modeling equation.\n",
    "- Finally, the predicted and the true peak values were compared for the entire window and a mean absolute error was used to give an estimate of the imputation efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A sample plot showing the training and validation losses with the early stopping mechanism in play is depicted here.\n",
    "![training_losses](LossesComparisonsForOutputLength=1OverEpochs.png)\n",
    "***The training and validation losses show good convergence after only 15 or 20 epochs.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The comparisons of the LSTM model with other imputation approaches are shown in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
