# Project Structure - Student Engagement Analysis

## ğŸ“ Repository Organization

```
student-engagement-analysis/
â”‚
â”œâ”€â”€ ğŸ“Š Data/
â”‚   â”œâ”€â”€ SQL/
â”‚   â”‚   â””â”€â”€ studentEngagement.sql
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ minutes_watched_2021_paid_1.csv
â”‚   â”‚   â”œâ”€â”€ minutes_watched_2021_paid_0.csv
â”‚   â”‚   â”œâ”€â”€ minutes_watched_2022_paid_1.csv
â”‚   â”‚   â”œâ”€â”€ minutes_watched_2022_paid_0.csv
â”‚   â”‚   â””â”€â”€ minutes_and_certificates.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ minutes_watched_2021_paid_1_no_outliers.csv
â”‚       â”œâ”€â”€ minutes_watched_2021_paid_0_no_outliers.csv
â”‚       â”œâ”€â”€ minutes_watched_2022_paid_1_no_outliers.csv
â”‚       â””â”€â”€ minutes_watched_2022_paid_0_no_outliers.csv
â”‚
â”œâ”€â”€ ğŸ““ Notebooks/
â”‚   â””â”€â”€ FinalProject.ipynb
â”‚
â”œâ”€â”€ ğŸ“ˆ Results/
â”‚   â”œâ”€â”€ FinalProject.xlsx
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ engagement_distribution_2021.png
â”‚       â”œâ”€â”€ engagement_distribution_2022.png
â”‚       â”œâ”€â”€ regression_plot.png
â”‚       â””â”€â”€ outlier_comparison.png
â”‚
â”œâ”€â”€ ğŸ“„ Documentation/
â”‚   â”œâ”€â”€ README_STUDENT_ENGAGEMENT.md
â”‚   â”œâ”€â”€ QUICKSTART_STUDENT_ENGAGEMENT.md
â”‚   â””â”€â”€ PROJECT_STRUCTURE_STUDENT.md (this file)
â”‚
â””â”€â”€ ğŸ”§ Requirements/
    â””â”€â”€ requirements.txt
```

---

## ğŸ“Š Data Files Description

### SQL Script: `studentEngagement.sql`

**Purpose**: Complete database setup, query development, and data export workflow

**Size**: ~5 KB  
**Lines**: ~200  
**Database**: `data_scientist_project`

#### Database Schema

**Tables** (4 total):

1. **`student_info`**
   - Student demographic and registration data
   - Columns: `student_id` (PK), registration details

2. **`student_purchases`**
   - Subscription purchase and refund records
   - Columns: `purchase_id` (PK), `student_id` (FK), `plan_id`, `date_purchased`, `date_refunded`
   
3. **`student_video_watched`**
   - Individual video viewing sessions
   - Columns: `student_id` (FK), `video_id`, `date_watched`, `seconds_watched`
   - Volume: ~millions of records
   
4. **`student_certificates`**
   - Certificate completion records
   - Columns: `student_id` (FK), `certificate_id`, `certificate_type`, `date_issued`

#### Views Created (2 total):

**1. `subscription_info`**

```sql
CREATE VIEW subscription_info AS
    SELECT 
        purchase_id,
        student_id,
        plan_id,
        date_purchased AS date_start,
        CASE 
            WHEN date_refunded IS NULL THEN
                CASE plan_id
                    WHEN 0 THEN DATE_ADD(date_purchased, INTERVAL 1 MONTH)
                    WHEN 1 THEN DATE_ADD(date_purchased, INTERVAL 3 MONTH)
                    WHEN 2 THEN DATE_ADD(date_purchased, INTERVAL 12 MONTH)
                END
            ELSE date_refunded
        END AS date_end
    FROM student_purchases;
```

**Purpose**:
- Calculates subscription end dates based on plan type
- Handles refunds appropriately
- Provides clean date ranges for Q2 analysis

**Columns**:
- `purchase_id`: Unique purchase identifier
- `student_id`: Student reference
- `plan_id`: 0 (Monthly), 1 (Quarterly), 2 (Annual)
- `date_start`: Purchase date
- `date_end`: Calculated expiration or refund date

**2. `purchase_info`**

```sql
CREATE VIEW purchase_info AS
    SELECT 
        student_id,
        CASE
            WHEN date_end < '2021-04-01' THEN 0
            WHEN date_start > '2021-06-30' THEN 0
            ELSE 1
        END AS paid_q2_2021,
        CASE
            WHEN date_end < '2022-04-01' THEN 0
            WHEN date_start > '2022-06-30' THEN 0
            ELSE 1
        END AS paid_q2_2022
    FROM subscription_info;
```

**Purpose**:
- Flags whether student had active subscription during Q2 of each year
- Binary indicator (0 = free, 1 = paid)
- Enables easy filtering for paid vs unpaid analysis

**Columns**:
- `student_id`: Student identifier
- `paid_q2_2021`: Binary flag for Q2 2021 subscription
- `paid_q2_2022`: Binary flag for Q2 2022 subscription

#### Key SQL Queries

**Query 1: Calculate Minutes Watched by Year**

```sql
SELECT 
    student_id,
    ROUND(SUM(seconds_watched) / 60, 2) AS minutes_watched
FROM student_video_watched
WHERE YEAR(date_watched) = 2021  -- or 2022
GROUP BY student_id;
```

**Operations**:
- Aggregates all viewing sessions per student
- Converts seconds to minutes
- Rounds to 2 decimal places
- Filters by calendar year

**Query 2: Join Minutes with Payment Status**

```sql
SELECT 
    min_watched.student_id,
    min_watched.minutes_watched,
    IF(income.date_start IS NULL, 0, MAX(income.paid_q2_2021)) AS paid_in_q2
FROM (
    SELECT student_id, ROUND(SUM(seconds_watched)/60, 2) AS minutes_watched
    FROM student_video_watched
    WHERE YEAR(date_watched) = 2021
    GROUP BY student_id
) min_watched
LEFT JOIN purchase_info income 
    ON min_watched.student_id = income.student_id
GROUP BY student_id
HAVING paid_in_q2 = 1;  -- Filter for paid or free (0)
```

**Operations**:
- Subquery calculates minutes per student
- LEFT JOIN preserves all students (even non-paying)
- IF statement handles students with no purchases
- MAX handles multiple purchase records
- HAVING clause filters by subscription status

**Query 3: Certificate Analysis**

```sql
SELECT 
    cert.student_id,
    cert.num_certificates,
    IFNULL(vid.minutes_watched, 0) AS minutes_watched
FROM (
    SELECT student_id, COUNT(student_id) AS num_certificates
    FROM student_certificates
    GROUP BY student_id
) cert
LEFT JOIN (
    SELECT student_id, ROUND(SUM(seconds_watched/60), 2) AS minutes_watched
    FROM student_video_watched
    GROUP BY student_id
) vid ON cert.student_id = vid.student_id
ORDER BY cert.student_id;
```

**Operations**:
- Counts certificates per student
- Joins with total viewing time
- IFNULL handles students with certificates but no viewing data
- Ordered output for easier review

**Query 4: Probability Analysis**

```sql
-- Event A: Students watching in Q2 2021
SELECT COUNT(DISTINCT student_id)
FROM student_video_watched
WHERE YEAR(date_watched) = 2021;

-- Event B: Students watching in Q2 2022
SELECT COUNT(DISTINCT student_id)
FROM student_video_watched
WHERE YEAR(date_watched) = 2022;

-- Event C: Students watching in both Q2 2021 AND Q2 2022
SELECT COUNT(DISTINCT student_id)
FROM (
    SELECT DISTINCT student_id FROM student_video_watched 
    WHERE YEAR(date_watched) = 2021
) a
JOIN (
    SELECT DISTINCT student_id FROM student_video_watched
    WHERE YEAR(date_watched) = 2022
) b USING(student_id);
```

**Operations**:
- Three separate queries for probability events
- DISTINCT ensures each student counted once
- JOIN finds intersection (students in both years)
- Enables calculation of P(A), P(B), P(A âˆ© B)

---

### CSV Data Files

#### 1. `minutes_watched_2021_paid_1.csv`

**Description**: Paid subscribers' viewing time in Q2 2021

**Schema**:
| Column | Type | Description |
|--------|------|-------------|
| `student_id` | INT | Unique student identifier |
| `minutes_watched` | FLOAT | Total minutes watched in Q2 2021 |

**Statistics**:
- Rows: ~1,000
- Size: ~25 KB
- Mean: 360.10 minutes
- Median: 161.93 minutes
- Std Dev: 499.62 minutes
- Min: 0.05 minutes
- Max: 5,000+ minutes (before outlier removal)

**Use Cases**:
- Compare paid vs free engagement
- Year-over-year analysis (vs 2022)
- Outlier detection
- Distribution visualization

#### 2. `minutes_watched_2021_paid_0.csv`

**Description**: Free users' viewing time in Q2 2021

**Schema**: Same as paid file

**Statistics**:
- Rows: ~5,000
- Size: ~120 KB
- Mean: 14.21 minutes
- Median: 2.79 minutes
- Std Dev: 24.48 minutes
- Min: 0.05 minutes
- Max: 200+ minutes

**Observations**:
- Much larger sample size (5Ã— paid users)
- Heavily right-skewed distribution
- Lower engagement overall
- Many "taste testers" (< 5 minutes)

#### 3. `minutes_watched_2022_paid_1.csv`

**Description**: Paid subscribers' viewing time in Q2 2022

**Schema**: Same as above

**Statistics**:
- Rows: ~1,200
- Size: ~30 KB
- Mean: 292.22 minutes (-18.9% vs 2021)
- Median: 119.75 minutes (-26.0% vs 2021)
- Std Dev: 420.19 minutes
- Growth: +20% more paid users vs 2021

**Key Change**:
- Engagement declined year-over-year
- More users but less engagement per user
- Suggests possible content saturation or competition

#### 4. `minutes_watched_2022_paid_0.csv`

**Description**: Free users' viewing time in Q2 2022

**Schema**: Same as above

**Statistics**:
- Rows: ~4,800
- Size: ~115 KB
- Mean: 16.25 minutes (+14.1% vs 2021)
- Median: 5.02 minutes (+78.6% vs 2021)
- Std Dev: 24.83 minutes
- Slight decrease in total free users (-4%)

**Key Change**:
- Engagement increased year-over-year
- Median doubled (more engaged casual users)
- Better content discovery or improved UX

#### 5. `minutes_and_certificates.csv`

**Description**: Certificate completion data with viewing time

**Schema**:
| Column | Type | Description |
|--------|------|-------------|
| `student_id` | INT | Unique identifier |
| `num_certificates` | INT | Total certificates earned |
| `minutes_watched` | FLOAT | Total viewing time (all time) |

**Statistics**:
- Rows: ~500
- Size: ~12 KB
- Certificate range: 1-13
- Minutes range: 148 - 6,066
- Mean certificates: 3.2
- Mean minutes: 1,245

**Purpose**:
- Linear regression analysis
- Correlation between engagement and outcomes
- Predictive modeling

---

### Processed Files (Outlier-Filtered)

#### `*_no_outliers.csv` Files

**Purpose**: Cleaned datasets with extreme values removed

**Method**:
```python
# 99th percentile threshold
qv = df['minutes_watched'].quantile(q=0.99)
df_clean = df[df['minutes_watched'] <= qv]
```

**Impact**:
- Removes top 1% most extreme values
- Preserves 99% of data
- Improves statistical analysis robustness
- Reduces skewness in visualizations

**Files Generated**:
1. `minutes_watched_2021_paid_1_no_outliers.csv` (~990 rows)
2. `minutes_watched_2021_paid_0_no_outliers.csv` (~4,950 rows)
3. `minutes_watched_2022_paid_1_no_outliers.csv` (~1,188 rows)
4. `minutes_watched_2022_paid_0_no_outliers.csv` (~4,752 rows)

**Usage**:
- Hypothesis testing
- Cleaner visualizations
- More reliable mean/std calculations

---

## ğŸ““ Notebook: `FinalProject.ipynb`

**Purpose**: Complete Python analysis workflow from data loading to modeling

**Size**: ~200 KB  
**Cells**: ~26  
**Execution Time**: 5-10 minutes

### Notebook Structure

#### Section 1: Setup and Imports

```python
# Cell 1: Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()
```

**Libraries Used**:
- `pandas`: Data manipulation
- `matplotlib.pyplot`: Base plotting
- `seaborn`: Statistical visualizations
- `numpy`: Numerical operations (imported later)
- `scikit-learn`: Machine learning (imported later)

#### Section 2: Data Loading

```python
# Cell 2: Load CSV files
minutes_2021_paid = pd.read_csv("minutes_watched_2021_paid_1.csv")
minutes_2021_unpaid = pd.read_csv("minutes_watched_2021_paid_0.csv")
minutes_2022_paid = pd.read_csv("minutes_watched_2022_paid_1.csv")
minutes_2022_unpaid = pd.read_csv("minutes_watched_2022_paid_0.csv")
```

**Data Loaded**:
- 4 primary datasets (2 years Ã— 2 subscription types)
- Total rows: ~11,000+
- Columns: `student_id`, `minutes_watched`

**Quality Checks** (implied):
- No missing values in minutes_watched
- All values non-negative
- Student IDs are unique

#### Section 3: Exploratory Visualization - 2021

```python
# Cell 3: KDE plot for 2021 comparison
plt.figure(figsize=[10,10])
sns.kdeplot(
    data=minutes_2021_paid, 
    x="minutes_watched", 
    color="blue", 
    label="Students with paid subscription in Q2 of 2021"
)
sns.kdeplot(
    data=minutes_2021_unpaid,
    x="minutes_watched",
    color="red",
    label="Students with free subscription in Q2 of 2021"
)
plt.xlabel("Minutes watched")
plt.ylabel("Probability density")
plt.legend()
plt.show()
```

**Visualization Type**: Kernel Density Estimate (KDE)

**Purpose**:
- Compare paid vs free engagement distributions
- Identify distribution shapes (skewness)
- Spot modal behaviors

**Observations** (from markdown cell 4):
- Paid users: Right-skewed, most watch moderately
- Free users: Heavily left-skewed, most watch very little
- Clear separation between groups
- Suggests subscription strongly influences engagement

#### Section 4: Exploratory Visualization - 2022

```python
# Cell 5: KDE plot for 2022 comparison
# Similar structure to 2021 plot
```

**Observations** (from markdown cell 6):
- Similar patterns to 2021
- Fewer paid users with very high engagement
- Slightly more engaged free users
- Overall distributions maintain shape

#### Section 5: Outlier Detection

```python
# Cell 7: Calculate 99th percentile thresholds
qv_2021_paid = minutes_2021_paid["minutes_watched"].quantile(q=0.99)
qv_2021_unpaid = minutes_2021_unpaid["minutes_watched"].quantile(q=0.99)
qv_2022_paid = minutes_2022_paid["minutes_watched"].quantile(q=0.99)
qv_2022_unpaid = minutes_2022_unpaid["minutes_watched"].quantile(q=0.99)

# Filter datasets
minutes_99_paid_2021 = minutes_2021_paid[
    minutes_2021_paid["minutes_watched"] <= qv_2021_paid
]
# ... (similar for other datasets)
```

**Methodology**:
- Calculate 99th percentile for each dataset
- Filter out values above threshold
- Preserve 99% of observations

**Rationale**:
- Extreme outliers can distort analyses
- Maintain statistical validity
- Improve visualization clarity

#### Section 6: Outlier-Filtered Visualizations

```python
# Cells 8-9: Re-plot distributions without outliers
```

**Purpose**:
- Verify outlier removal effectiveness
- Generate cleaner visualizations
- Prepare data for statistical testing

#### Section 7: Export Cleaned Data

```python
# Cell 10: Save processed files
minutes_99_paid_2021.to_csv("minutes_watched_2021_paid_1_no_outliers.csv", index=False)
minutes_99_unpaid_2021.to_csv("minutes_watched_2021_paid_0_no_outliers.csv", index=False)
# ... (similar for 2022)
```

**Output**:
- 4 cleaned CSV files
- Ready for external analysis tools
- Documented preprocessing steps

#### Section 8: Machine Learning Setup

```python
# Cell 11: Import ML libraries
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import numpy as np
```

**Libraries**:
- `LinearRegression`: Simple linear model
- `train_test_split`: Data partitioning
- `numpy`: Array operations

#### Section 9: Load Certificate Data

```python
# Cell 12: Load and inspect
certificate_data = pd.read_csv("minutes_and_certificates.csv")
certificate_data.head()
```

**Dataset**: ~500 students with certificates
**Columns**: student_id, num_certificates, minutes_watched

#### Section 10: Feature Engineering

```python
# Cell 13-14: Prepare features
data = certificate_data.copy()
data_x = data["minutes_watched"]      # Feature
data_y = data["num_certificates"]     # Target

# Cell 15: Train-test split
train_x, val_x, train_y, val_y = train_test_split(
    data_x, data_y, 
    test_size=0.2, 
    random_state=365
)

# Cell 16: Reshape for sklearn
train_X = np.array(train_x).reshape(len(train_x), 1)
train_Y = np.array(train_y).reshape(len(train_y), 1)
test_X = np.array(val_x).reshape(len(val_x), 1)
test_Y = np.array(val_y).reshape(len(val_y), 1)
```

**Split**:
- 80% training (~400 samples)
- 20% validation (~100 samples)
- Random state for reproducibility

**Shape**:
- X: (n_samples, 1) - single feature
- Y: (n_samples, 1) - single target

#### Section 11: Model Training

```python
# Cell 17-18: Initialize and fit
model = LinearRegression()
model.fit(train_X, train_Y)

# Cell 19: Display parameters
print(f"Linear regression model intercept is: {model.intercept_}")
print(f"Linear regression model slope is: {model.coef_}")
```

**Model**: Simple Linear Regression
**Equation**: y = mx + b
**Parameters**:
- Intercept (b): 0.82
- Slope (m): 0.0022

**Interpretation**:
- Base certificates (0 minutes): 0.82
- Each additional minute: +0.0022 certificates
- Each 100 minutes: +0.22 certificates

#### Section 12: Model Evaluation

```python
# Cell 20: Formatted equation
print(f"The equation of the linear model is: y = {round(model.coef_[0][0],4)}x + {round(model.intercept_[0],3)}")

# Cell 21: Correlation
correlation = np.corrcoef(train_X[:,0], train_Y[:,0])[0,1]
print(f"The correlation coefficient calculated by the model is: {round(correlation,4)}")
```

**Results**:
- Equation: `certificates = 0.0022 Ã— minutes + 0.82`
- Correlation: r = 0.71
- RÂ²: ~0.50 (rÂ² = 0.71Â² â‰ˆ 0.50)

**Interpretation**:
- Moderate-strong positive correlation
- Model explains ~50% of variance
- Significant relationship between viewing and certificates

#### Section 13: Predictions

```python
# Cell 22-23: Generate predictions
y_pred = model.predict(test_X)
y_train_pred = model.predict(train_X)
```

**Purpose**:
- Evaluate model performance
- Visualize predictions vs actuals
- Assess generalization to test set

#### Section 14: Visualization - Training Data

```python
# Cell 24: Scatter plot with fit line
plt.figure(figsize=[8,8])
plt.scatter(train_X, train_Y, color='b', label="Actual values")
plt.scatter(train_X, y_train_pred, color='r', label="Predicted values")
plt.xlabel("Minutes watched")
plt.ylabel("Number of certificates")
plt.title("Training Data: Actual vs Predicted")
plt.legend()
plt.show()
```

**Observation** (from markdown cell 25):
- Not a very striking relation (moderate scatter)
- Linear assumption holds reasonably well
- Some residual variance unexplained

#### Section 15: Visualization - Test Data

```python
# Cell 26: Test set predictions
plt.figure(figsize=[8,8])
plt.scatter(test_Y, y_pred, color='b')
plt.xlabel("Actual number of certificates")
plt.ylabel("Predicted number of certificates")
plt.title("Test Data: Actual vs Predicted")
plt.show()
```

**Purpose**:
- Evaluate generalization
- Check for overfitting
- Assess model robustness

#### Section 16: Example Prediction

```python
# Cell 27: Predict for 1200 minutes
y_pred_1200 = model.predict([[1200]])
print(y_pred_1200)
# Output: [[3.23809089]]
```

**Interpretation**:
- Student watching 1200 minutes
- Predicted: ~3.2 certificates
- Reasonable estimate based on data

---

## ğŸ“ˆ Results File: `FinalProject.xlsx`

**Purpose**: Excel summary of key findings and statistics

**Size**: ~50 KB  
**Sheets**: 6

### Sheet 1: `free_2021`

**Content**: Free users Q2 2021 statistics

**Layout**:
```
Column B-C: student_id, minutes_watched (sample data)
Column E-F: Summary statistics
  - Mean: 14.208262
  - Median: 2.791650
  - Std. Dev: 24.476627
```

**Purpose**: Quick reference for free user engagement in 2021

### Sheet 2: `paid_2021`

**Content**: Paid users Q2 2021 statistics

**Layout**:
```
Column B-C: student_id, minutes_watched (sample data)
Column E-F: Summary statistics
  - Mean: 360.103802
  - Median: 161.933300
  - Std. Dev: 499.616019
```

**Purpose**: Benchmark for paid user engagement

### Sheet 3: `free_2022`

**Content**: Free users Q2 2022 statistics

**Statistics**:
- Mean: 16.248823 (+14% vs 2021)
- Median: 5.016700 (+79% vs 2021)
- Std. Dev: 24.827161

**Insight**: Free users more engaged in 2022

### Sheet 4: `paid_2022`

**Content**: Paid users Q2 2022 statistics

**Statistics**:
- Mean: 292.220923 (-19% vs 2021)
- Median: 119.749950 (-26% vs 2021)
- Std. Dev: 420.185493

**Insight**: Paid users less engaged in 2022

### Sheet 5: `certificates`

**Content**: Sample certificate data

**Columns**:
- student_id
- num_certificates
- minutes_watched

**Purpose**: Illustrate relationship between viewing and completion

### Sheet 6: `probabilities`

**Content**: Event probability analysis

**Events**:
- Event A: Students watching videos in Q2 2021
- Event B: Students watching videos in Q2 2022
- Event C: Students watching in both quarters

**Purpose**: Retention and engagement overlap analysis

---

## ğŸ”§ Requirements File

### `requirements.txt`

```txt
# Data Processing
pandas>=1.3.0
numpy>=1.21.0

# Visualization
matplotlib>=3.4.0
seaborn>=0.11.0

# Machine Learning
scikit-learn>=1.0.0

# Database
mysql-connector-python>=8.0.0

# Jupyter
jupyter>=1.0.0
notebook>=6.4.0
```

**Installation**:
```bash
pip install -r requirements.txt
```

**Total Size**: ~150 MB installed

---

## ğŸ“Š Data Flow Diagram

```
MySQL Database (data_scientist_project)
â”œâ”€â”€ student_info
â”œâ”€â”€ student_purchases
â”œâ”€â”€ student_video_watched
â””â”€â”€ student_certificates
    â†“
[SQL Queries & Views]
â”œâ”€â”€ subscription_info (view)
â””â”€â”€ purchase_info (view)
    â†“
[CSV Exports]
â”œâ”€â”€ minutes_watched_2021_paid_1.csv
â”œâ”€â”€ minutes_watched_2021_paid_0.csv
â”œâ”€â”€ minutes_watched_2022_paid_1.csv
â”œâ”€â”€ minutes_watched_2022_paid_0.csv
â””â”€â”€ minutes_and_certificates.csv
    â†“
[Python Analysis (Jupyter)]
â”œâ”€â”€ Load data
â”œâ”€â”€ Exploratory analysis
â”œâ”€â”€ Outlier removal
â”œâ”€â”€ Visualization
â””â”€â”€ Predictive modeling
    â†“
[Outputs]
â”œâ”€â”€ Cleaned CSVs (*_no_outliers.csv)
â”œâ”€â”€ Figures (.png files)
â”œâ”€â”€ Excel summary (FinalProject.xlsx)
â””â”€â”€ Model predictions
```

---

## ğŸ¯ Analysis Phases

### Phase 1: SQL Data Extraction
**Time**: 30-60 minutes  
**Tools**: MySQL, MySQL Workbench  
**Output**: 5 CSV files

### Phase 2: Exploratory Analysis
**Time**: 30-45 minutes  
**Tools**: Python, pandas, matplotlib  
**Output**: Distribution plots, summary statistics

### Phase 3: Data Cleaning
**Time**: 15-20 minutes  
**Tools**: Python, pandas  
**Output**: Outlier-filtered datasets

### Phase 4: Statistical Analysis
**Time**: 20-30 minutes  
**Tools**: Python, scipy (implied)  
**Output**: Hypothesis tests, probability calculations

### Phase 5: Predictive Modeling
**Time**: 20-30 minutes  
**Tools**: Python, scikit-learn  
**Output**: Linear regression model, predictions

### Phase 6: Reporting
**Time**: 30-45 minutes  
**Tools**: Excel, Jupyter  
**Output**: FinalProject.xlsx, documented notebook

**Total Time**: 2.5-3.5 hours

---

## ğŸ“š Key Concepts Demonstrated

### SQL Skills
âœ… Complex joins (LEFT JOIN, INNER JOIN)  
âœ… Subqueries in SELECT and FROM  
âœ… CASE statements for conditional logic  
âœ… Date functions (DATE_ADD, YEAR)  
âœ… Aggregate functions (SUM, COUNT, MAX)  
âœ… View creation and management  
âœ… Data export strategies

### Python Skills
âœ… pandas DataFrames  
âœ… Data loading and inspection  
âœ… Grouping and aggregation  
âœ… Outlier detection methods  
âœ… Train-test splitting  
âœ… Model fitting and evaluation

### Statistical Skills
âœ… Descriptive statistics  
âœ… Distribution analysis  
âœ… Quantile calculations  
âœ… Correlation analysis  
âœ… Hypothesis testing (implied)  
âœ… Probability calculations

### Machine Learning Skills
âœ… Linear regression  
âœ… Feature engineering  
âœ… Model training  
âœ… Prediction generation  
âœ… Model interpretation

### Visualization Skills
âœ… KDE plots  
âœ… Scatter plots  
âœ… Multi-panel figures  
âœ… Clear labeling  
âœ… Appropriate color schemes

---

## ğŸ” Model Details

### Linear Regression Model

**Type**: Simple Linear Regression  
**Library**: scikit-learn  
**Class**: `LinearRegression`

**Architecture**:
```
Input: minutes_watched (single feature)
  â†“
Linear transformation: y = mx + b
  â†“
Output: num_certificates (continuous)
```

**Training**:
```python
model = LinearRegression()
model.fit(train_X, train_Y)
```

**Parameters Learned**:
- Slope (m): 0.0022
- Intercept (b): 0.82

**Assumptions**:
- Linear relationship between X and y
- Homoscedasticity (constant variance)
- Independence of observations
- Normal distribution of residuals (for inference)

**Limitations**:
- Only explains ~50% of variance
- Assumes linear relationship (may be non-linear)
- Does not account for:
  - Student background
  - Course difficulty
  - Learning style
  - Time of enrollment

**Potential Improvements**:
1. Polynomial features (non-linear relationships)
2. Additional features (demographics, course type)
3. Regularization (Ridge, Lasso)
4. Ensemble methods (Random Forest, XGBoost)

---

## ğŸ’¡ Business Insights

### For Platform Management

**Finding 1: Paid engagement declined 19%**
- **Impact**: Revenue retention risk
- **Action**: Investigate content freshness
- **Metric**: Monthly active paid users

**Finding 2: Free engagement increased 14%**
- **Impact**: Positive onboarding signal
- **Action**: Conversion funnel optimization
- **Metric**: Free-to-paid conversion rate

**Finding 3: Certificates correlate with viewing (r=0.71)**
- **Impact**: Validates content quality
- **Action**: Maintain instructional standards
- **Metric**: Completion rates by course

### For Marketing

**Opportunity**: Convert engaged free users
- Target: Free users with >100 minutes watched
- Message: "You're already learning, unlock full access"
- Channel: In-app prompts, email campaigns

**Risk**: Paid user churn
- Monitor: Engagement drop triggers
- Intervention: Personalized content recommendations
- Retention: Loyalty programs, exclusive content

---

**Last Updated**: November 2024  
**Version**: 1.0.0  
**Maintainer**: Samarpan Chakraborty (schakr18@umd.edu)